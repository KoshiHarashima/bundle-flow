# scripts/train_stage2.py
# BundleFlow Stage 2: Menu Optimization
# å‚ç…§: æœŸå¾…åŠ¹ç”¨ã®ç­‰ä¾¡è¡¨ç¾ Eq.(19), ODEè§£ Eq.(20), å³å¯†åŠ¹ç”¨ã®æœ‰é™æ”¯æŒç‰ˆ Eq.(21),
#       åç›Šæœ€å¤§åŒ–æå¤± Eq.(22), SoftMax å‰²å½“ Eq.(23) ã€‚:contentReference[oaicite:2]{index=2} :contentReference[oaicite:3]{index=3}

import os, time, argparse, random
from typing import List
import torch
import torch.nn as nn
import torch.optim as optim

from bf.flow import FlowModel
from bf.menu import MenuElement, make_null_element, revenue_loss, utilities_matrix
from bf.data import load_cats_dir, train_test_split, gen_uniform_iid_xor
from bf.valuation import XORValuation

# ---------- utilsï¼ˆæœ€å°é™ï¼‰ ----------
def seed_all(seed: int):
    random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def lambda_schedule(step: int, total: int, start: float = 1e-3, end: float = 0.2) -> float:
    # SoftMaxæ¸©åº¦ Î» ã‚’ç·šå½¢ã§ Eq.(23) ã«æŠ•å…¥ï¼ˆSetupå‚ç…§ï¼‰ã€‚:contentReference[oaicite:4]{index=4}
    alpha = min(max(step / max(total, 1), 0.0), 1.0)
    return (1 - alpha) * start + alpha * end

def freeze_module(m: nn.Module):
    for p in m.parameters():
        p.requires_grad_(False)
    m.eval()

# ---------- menu æ§‹ç¯‰ ----------
def build_menu(m: int, K: int, D: int) -> List[MenuElement]:
    menu = [MenuElement(m, D) for _ in range(K)]
    # IRç”¨ãƒŒãƒ«è¦ç´ ï¼ˆã‚¼ãƒ­é…åˆ†ãƒ»ã‚¼ãƒ­ä¾¡æ ¼ï¼‰
    menu.append(make_null_element(m))
    return menu

# ---------- Î¼ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆ ----------
@torch.no_grad()
def warmstart_mus(flow: FlowModel, menu: List[MenuElement], t_grid: torch.Tensor, 
                  n_grid: int = 100, seed: int = 42):
    """
    ãƒ•ãƒ­ãƒ¼ã§ä»£è¡¨æŸã‚’ç”Ÿæˆã—ã€Î¼ã‚’åˆæœŸåŒ–ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆï¼‰
    åŠ¹æœï¼šç•°ãªã‚‹æŸé ˜åŸŸã«Î¼ã‚’æ•£ã‚‰ã—ã€åˆæœŸåæŸã‚’åŠ é€Ÿ
    """
    device = t_grid.device
    m = flow.m
    torch.manual_seed(seed)
    
    # ãƒ©ãƒ³ãƒ€ãƒ ãªÎ¼ã‚°ãƒªãƒƒãƒ‰ã‚’ç”Ÿæˆï¼ˆStage1ã¨åŒã˜ç¯„å›²: [-0.2, 1.2]ï¼‰
    mu_grid = torch.rand(n_grid, m, device=device) * 1.4 - 0.2  # [0,1] â†’ [-0.2, 1.2]
    
    # flow_forward â†’ round ã—ã¦ä»£è¡¨æŸã‚’å¾—ã‚‹
    sT_grid = flow.flow_forward(mu_grid, t_grid)  # (n_grid, m)
    bundles = flow.round_to_bundle(sT_grid)       # (n_grid, m)
    
    # é‡è¤‡ã‚’å‰Šé™¤ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãªæŸã®ã¿ï¼‰
    bundles_unique = torch.unique(bundles, dim=0)
    print(f"[WarmStart] Generated {len(bundles_unique)} unique bundles from {n_grid} samples")
    
    # å„ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã®Î¼ã‚’ä»£è¡¨æŸã‹ã‚‰åˆæœŸåŒ–
    for k, elem in enumerate(menu[:-1]):  # æœ€å¾Œã®nullè¦ç´ ã‚’é™¤ã
        D = elem.mus.shape[0]
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ä»£è¡¨æŸã‚’é¸æŠã—ã¦Î¼ã¨ã—ã¦é…ç½®
        if len(bundles_unique) >= D:
            idx = torch.randperm(len(bundles_unique))[:D]
            elem.mus.data.copy_(bundles_unique[idx])
        else:
            # ä»£è¡¨æŸãŒè¶³ã‚Šãªã„å ´åˆã¯ç¹°ã‚Šè¿”ã—ä½¿ç”¨
            idx = torch.randint(0, len(bundles_unique), (D,))
            elem.mus.data.copy_(bundles_unique[idx])
        
        # å°ã•ãªãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼ˆæ¢ç´¢ã®ãŸã‚ï¼‰
        elem.mus.data += 0.05 * torch.randn_like(elem.mus.data)
        elem.mus.data.clamp_(-0.2, 1.2)  # Stage1ã¨åŒã˜ç¯„å›²
    
    print(f"[WarmStart] Initialized Î¼ for {len(menu)-1} menu elements")

# ---------- å¼±ã„å†åˆæœŸåŒ– ----------
@torch.no_grad()
def reinit_unused_elements(flow: FlowModel, menu: List[MenuElement], Z: torch.Tensor,
                           t_grid: torch.Tensor, threshold: float = 0.01):
    """
    é¸æŠç¢ºç‡ãŒä½ã„è¦ç´ ã®Î¼ã‚’å†åˆæœŸåŒ–ï¼ˆæ¢ç´¢ã®è³ªã‚’ç¶­æŒï¼‰
    Z: (B, K+1) ã®é¸æŠç¢ºç‡è¡Œåˆ—
    threshold: ã“ã®å€¤æœªæº€ã®å¹³å‡é¸æŠç¢ºç‡ã‚’æŒã¤è¦ç´ ã‚’å†åˆæœŸåŒ–
    """
    device = t_grid.device
    m = flow.m
    
    # å„è¦ç´ ã®å¹³å‡é¸æŠç¢ºç‡ã‚’è¨ˆç®—
    z_mean = Z[:, :-1].mean(dim=0)  # (K,) æœ€å¾Œã®nullè¦ç´ ã‚’é™¤ã
    
    # é–¾å€¤æœªæº€ã®è¦ç´ ã‚’ç‰¹å®š
    unused_mask = z_mean < threshold
    n_unused = unused_mask.sum().item()
    
    if n_unused > 0:
        print(f"[ReInit] Found {n_unused} unused elements (z_mean < {threshold}), reinitializing...")
        
        # ä»£è¡¨æŸã‚’ç”Ÿæˆï¼ˆStage1ã¨åŒã˜ç¯„å›²ï¼‰
        mu_grid = torch.rand(50, m, device=device) * 1.4 - 0.2  # [-0.2, 1.2]
        sT_grid = flow.flow_forward(mu_grid, t_grid)
        bundles = flow.round_to_bundle(sT_grid)
        bundles_unique = torch.unique(bundles, dim=0)
        
        # æœªä½¿ç”¨è¦ç´ ã®Î¼ã‚’å†åˆæœŸåŒ–
        for k, elem in enumerate(menu[:-1]):
            if unused_mask[k]:
                D = elem.mus.shape[0]
                if len(bundles_unique) >= D:
                    idx = torch.randperm(len(bundles_unique))[:D]
                    elem.mus.data.copy_(bundles_unique[idx])
                else:
                    idx = torch.randint(0, len(bundles_unique), (D,))
                    elem.mus.data.copy_(bundles_unique[idx])
                
                # ãƒã‚¤ã‚ºè¿½åŠ 
                elem.mus.data += 0.05 * torch.randn_like(elem.mus.data)
                elem.mus.data.clamp_(-0.2, 1.2)  # Stage1ã¨åŒã˜ç¯„å›²
                
                # ä¾¡æ ¼Î²ã‚‚è»½ãå†åˆæœŸåŒ–
                elem.beta.data.fill_(0.1 * torch.randn(1).item())
    
    return n_unused

# ---------- ãƒ‡ãƒ¼ã‚¿ ----------
def make_dataset(args) -> List[XORValuation]:
    if args.cats_glob:
        V = load_cats_dir(args.cats_glob, m=args.m, keep_dummy=None, max_files=args.max_files, shuffle=True)
    else:
        # åˆæˆXORï¼ˆTable 4 ã®åŸå­æ•° a ã‚’æ¨¡å€£ï¼‰ã‚’ N æœ¬ç”Ÿæˆã€‚:contentReference[oaicite:5]{index=5}
        V = [gen_uniform_iid_xor(args.m, a=args.a, low=0.0, high=1.0, seed=1337 + i, 
                                 atom_size_mode=args.atom_size_mode) for i in range(args.n_val)]
    return V

# ---------- ãƒ‡ãƒã‚¤ã‚¹æœ€é©åŒ–æ©Ÿèƒ½ ----------
def get_optimal_device(args):
    """
    ãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•é¸æŠã—ã€æœ€é©åŒ–è¨­å®šã‚’é©ç”¨
    """
    if torch.cuda.is_available() and not args.cpu:
        device = torch.device("cuda")
        
        # GPUæƒ…å ±ã‚’è¡¨ç¤º
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"[Stage2] ğŸš€ Using GPU: {gpu_name}", flush=True)
        print(f"[Stage2] ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB", flush=True)
        
        # Colab A100ç”¨ã®æœ€é©åŒ–è¨­å®š
        if "A100" in gpu_name:
            print(f"[Stage2] âš¡ Colab A100 detected! Applying maximum optimizations...", flush=True)
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.empty_cache()
            
            # A100ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•èª¿æ•´
            if not hasattr(args, 'auto_optimize') or args.auto_optimize:
                print(f"[Stage2] ğŸ”§ Auto-optimizing parameters for Colab A100...", flush=True)
                args.batch = min(args.batch * 8, 2048)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’8å€ã«
                args.K = min(args.K * 4, 4096)  # ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ æ•°ã‚’4å€ã«
                args.D = min(args.D * 4, 64)  # ç‰¹å¾´æ¬¡å…ƒã‚’4å€ã«
                print(f"[Stage2] ğŸ“Š Optimized: batch={args.batch}, K={args.K}, D={args.D}", flush=True)
                
        elif "V100" in gpu_name or "H100" in gpu_name:
            print(f"[Stage2] âš¡ High-end GPU detected! Applying optimizations...", flush=True)
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.empty_cache()
            
            # é«˜æ€§èƒ½GPUç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
            if not hasattr(args, 'auto_optimize') or args.auto_optimize:
                print(f"[Stage2] ğŸ”§ Auto-optimizing parameters for {gpu_name}...", flush=True)
                args.batch = min(args.batch * 4, 1024)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’4å€ã«
                args.K = min(args.K * 2, 2048)  # ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ æ•°ã‚’2å€ã«
                args.D = min(args.D * 2, 32)  # ç‰¹å¾´æ¬¡å…ƒã‚’2å€ã«
                print(f"[Stage2] ğŸ“Š Optimized: batch={args.batch}, K={args.K}, D={args.D}", flush=True)
        
        elif "T4" in gpu_name or "K80" in gpu_name:
            print(f"[Stage2] âš ï¸  Mid-range GPU detected. Using conservative settings...", flush=True)
            torch.backends.cudnn.benchmark = True
            torch.cuda.empty_cache()
            
        else:
            print(f"[Stage2] ğŸ”§ Standard GPU detected. Using default optimizations...", flush=True)
            torch.backends.cudnn.benchmark = True
            torch.cuda.empty_cache()
            
    else:
        device = torch.device("cpu")
        print(f"[Stage2] ğŸ’» Using CPU (GPU not available or --cpu flag set)", flush=True)
        print(f"[Stage2] âš ï¸  CPU mode is very slow. Consider using GPU for better performance.", flush=True)
        
        # CPUç”¨ã®æœ€é©åŒ–
        if not hasattr(args, 'auto_optimize') or args.auto_optimize:
            print(f"[Stage2] ğŸ”§ Auto-optimizing parameters for CPU...", flush=True)
            args.batch = min(args.batch, 64)  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åˆ¶é™
            args.K = min(args.K, 256)  # ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ æ•°ã‚’åˆ¶é™
            args.D = min(args.D, 8)  # ç‰¹å¾´æ¬¡å…ƒã‚’åˆ¶é™
            print(f"[Stage2] ğŸ“Š Optimized: batch={args.batch}, K={args.K}, D={args.D}", flush=True)
    
    return device

# ---------- å­¦ç¿’æœ¬ä½“ï¼ˆEq.(21)â†’(22) æœ€é©åŒ–ï¼‰ ----------
def train_stage2(args):
    # è‡ªå‹•æœ€é©åŒ–ã®åˆ¶å¾¡
    if args.no_auto_optimize:
        args.auto_optimize = False
    
    # ãƒ‡ãƒã‚¤ã‚¹åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½
    device = get_optimal_device(args)
    seed_all(args.seed)

    # Stage 1 ã®ãƒ•ãƒ­ãƒ¼èª­è¾¼ï¼ˆÏ† å›ºå®šï¼›Eq.(20)ã§ä½¿ç”¨ï¼‰ :contentReference[oaicite:6]{index=6}
    print(f"[Stage2] Loading checkpoint: {args.flow_ckpt}", flush=True)
    if os.path.exists(args.flow_ckpt):
        file_size = os.path.getsize(args.flow_ckpt) / (1024 * 1024)  # MB
        print(f"[Stage2] Checkpoint file size: {file_size:.2f} MB", flush=True)
    else:
        print(f"[Stage2] WARNING: Checkpoint file not found: {args.flow_ckpt}", flush=True)
    
    print(f"[Stage2] Starting torch.load()...", flush=True)
    ckpt = torch.load(args.flow_ckpt, map_location=device)
    print(f"[Stage2] Checkpoint loaded successfully", flush=True)
    print(f"[Stage2] Checkpoint keys: {list(ckpt.keys())}", flush=True)
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ­£è¦åŒ–ã®æœ‰ç„¡ã‚’åˆ¤å®š
    print(f"[Stage2] Checking for spectral normalization...", flush=True)
    use_spectral_norm = any('weight_orig' in k for k in ckpt["model"].keys())
    print(f"[Stage2] use_spectral_norm = {use_spectral_norm}", flush=True)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨åŒã˜è¨­å®šã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print(f"[Stage2] Creating FlowModel with m={ckpt['m']}, use_spectral_norm={use_spectral_norm}", flush=True)
    flow = FlowModel(m=ckpt["m"], use_spectral_norm=use_spectral_norm).to(device)
    print(f"[Stage2] Loading model state_dict...", flush=True)
    flow.load_state_dict(ckpt["model"])
    print(f"[Stage2] Freezing FlowModel parameters...", flush=True)
    freeze_module(flow)  # Ï† ã‚’å›ºå®šï¼ˆStage 2ï¼‰
    print(f"[Stage2] FlowModel ready (frozen)", flush=True)

    # ãƒ¡ãƒ‹ãƒ¥ãƒ¼åˆæœŸåŒ–ï¼ˆå„è¦ç´ ã¯ä¾¡æ ¼ Î² ã¨ Diracæ··åˆ Î±0ï¼›Eq.(21)ï¼‰ :contentReference[oaicite:7]{index=7}
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆ
    start_iter = 1
    loaded_ema = None
    if args.resume_ckpt:
        print(f"[Stage2] Resuming from checkpoint: {args.resume_ckpt}", flush=True)
        resume_data = torch.load(args.resume_ckpt, map_location=device)
        start_iter = resume_data.get("iteration", 0) + 1
        loaded_ema = resume_data.get("ema", None)
        print(f"[Stage2] Resuming from iteration {start_iter}, ema={loaded_ema}", flush=True)
    
    menu = build_menu(args.m, args.K, args.D)
    # ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã‚’deviceã«ç§»å‹•
    for elem in menu:
        elem.to(device)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
    if args.resume_ckpt:
        for i, elem in enumerate(menu):
            if i < len(resume_data["menu"]):
                beta_raw, logits, mus = resume_data["menu"][i]
                elem.beta_raw.data.copy_(beta_raw.to(device))
                elem.logits.data.copy_(logits.to(device))
                elem.mus.data.copy_(mus.to(device))
        print(f"[Stage2] Restored menu parameters from checkpoint", flush=True)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’Î¼/w ã¨Î²ã«åˆ†é›¢
    mu_w_params = []
    beta_params = []
    for elem in menu:
        for name, p in elem.named_parameters():
            if p.requires_grad:
                if 'beta' in name:
                    beta_params.append(p)
                else:  # mus, logits
                    mu_w_params.append(p)
    
    # æœ€åˆã¯Î¼/wã®ã¿å­¦ç¿’ï¼ˆÎ²ã¯å‡çµï¼‰
    opt = optim.Adam(mu_w_params, lr=args.lr)  # Setup: lrâ‰ˆ0.3 æ¨å¥¨ã€‚:contentReference[oaicite:8]{index=8}
    
    print(f"[Stage2] Warmup phase: Optimizing {len(mu_w_params)} Î¼/w parameters, {len(beta_params)} Î² parameters frozen", flush=True)

    # ä¾¡å€¤é–¢æ•°ãƒ‡ãƒ¼ã‚¿ï¼ˆV ã‚’ train/test ã«åˆ†å‰²ï¼‰ :contentReference[oaicite:9]{index=9}
    V_all = make_dataset(args)
    
    # ãƒ‡ãƒãƒƒã‚°ï¼šæœ€åˆã®valuationã‚’è©³ã—ãç¢ºèª
    if len(V_all) > 0:
        print(f"[Stage2] Dataset check: {len(V_all)} valuations created", flush=True)
        v0 = V_all[0]
        print(f"[Stage2] First valuation: v0.m = {v0.m}", flush=True)
        print(f"[Stage2] First valuation has {len(v0.atoms)} atoms", flush=True)
        if len(v0.atoms) > 0:
            atom0_mask, atom0_price = v0.atoms[0]
            atom0_bits = bin(atom0_mask).count('1')
            atom0_highest = atom0_mask.bit_length() - 1
            print(f"[Stage2] First atom: {atom0_bits} items, price={atom0_price:.4f}, mask={atom0_mask}, highest_bit={atom0_highest}", flush=True)
            # ãƒ†ã‚¹ãƒˆï¼šå…¨ã¦ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æŒã¤bundleã®ä¾¡å€¤
            all_items_bundle = torch.ones(args.m, device=device)
            test_val = v0.value(all_items_bundle)
            print(f"[Stage2] Value of bundle with ALL {args.m} items: {test_val:.4f}", flush=True)
            if test_val == 0.0:
                print(f"[Stage2] WARNING: Even with all items, value is 0! This suggests a bug.", flush=True)
            
            # ã•ã‚‰ã«ï¼šv0.m ã¨args.mã®æ¯”è¼ƒ
            if hasattr(v0, 'm') and v0.m != args.m:
                print(f"[Stage2] ERROR: Valuation.m ({v0.m}) != args.m ({args.m})!", flush=True)
    
    train, test = train_test_split(V_all, train_ratio=0.95, seed=args.seed)
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šv0ã‚’ä¿å­˜ï¼ˆèµ·å‹•æ™‚ãƒ†ã‚¹ãƒˆã§æˆåŠŸã—ãŸvaluationï¼‰
    v0_for_debug = V_all[0]

    # æ™‚é–“ã‚°ãƒªãƒƒãƒ‰ï¼ˆEq.(12),(20) ã®é›¢æ•£åŒ–ï¼‰
    t_grid = torch.linspace(0.0, 1.0, steps=args.ode_steps, device=device)
    
    # Î¼ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆï¼ˆä»£è¡¨æŸã‹ã‚‰åˆæœŸåŒ–ï¼‰
    if args.warmstart:
        warmstart_mus(flow, menu, t_grid, n_grid=args.warmstart_grid, seed=args.seed)

    # ãƒ«ãƒ¼ãƒ—
    ema = loaded_ema if loaded_ema is not None else None
    t0 = time.time()
    beta_unfrozen = False  # Î²ã®å‡çµçŠ¶æ…‹ã‚’ç®¡ç†
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹å ´åˆã€å‡çµçŠ¶æ…‹ã‚‚å¾©å…ƒ
    if args.resume_ckpt and start_iter > args.freeze_beta_iters:
        beta_unfrozen = True
        all_params = mu_w_params + beta_params
        opt = optim.Adam(all_params, lr=args.lr)
        if "optimizer_state" in resume_data:
            opt.load_state_dict(resume_data["optimizer_state"])
        print(f"[Stage2] Resumed with Î² unfrozen", flush=True)
    
    for it in range(start_iter, args.iters + 1):
        # Î²ã®å‡çµè§£é™¤ï¼ˆwarmupæœŸé–“å¾Œï¼‰
        if not beta_unfrozen and it > args.freeze_beta_iters:
            print(f"\n[Iteration {it}] Unfreezing Î² parameters...", flush=True)
            all_params = mu_w_params + beta_params
            opt = optim.Adam(all_params, lr=args.lr)
            beta_unfrozen = True
            print(f"[Iteration {it}] Now optimizing {len(all_params)} parameters (Î¼/w/Î²)", flush=True)
        # GPUæœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†
        B = min(args.batch, len(train))
        batch = random.sample(train, B)
        
        # GPU ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆå®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
        if it % 100 == 0 and torch.cuda.is_available():
            torch.cuda.empty_cache()

        # Î»ï¼ˆEq.(23)ï¼‰ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
        lam = lambda_schedule(it, args.iters, start=args.lam_start, end=args.lam_end)
        
        # Gumbel-Softmaxæ¸©åº¦ã‚’ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ï¼ˆuse_gumbelã®æ™‚ã®ã¿ä½¿ç”¨ï¼‰
        if args.use_gumbel:
            tau = args.tau_start * (args.tau_end / args.tau_start) ** ((it - 1) / args.iters)
        else:
            tau = 0.1  # ãƒ€ãƒŸãƒ¼ï¼ˆä½¿ç”¨ã•ã‚Œãªã„ï¼‰

        # åç›Šæå¤±ï¼ˆEq.(22)ï¼‰ã‚’æœ€å°åŒ–ã€‚å†…éƒ¨ã§åŠ¹ç”¨ u^(k)(v) ã‚’ Eq.(21) ã§å³å¯†è¨ˆç®—ã€‚
        # ãƒ­ã‚°å‡ºåŠ›æ™‚ã«ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        is_log_iter = (it % args.log_every == 0)
        verbose = (it == 1)
        if verbose:
            method_str = "Gumbel-Softmax+STE" if args.use_gumbel else "Softmax"
            print(f"\n[Iteration {it}] Starting forward pass ({method_str}, tau={tau:.4f})...", flush=True)
        
        # ãƒ¡ãƒ‹ãƒ¥ãƒ¼å¯è¦–åŒ–ï¼ˆæœ€åˆã®iterationã¨å®šæœŸçš„ã«ï¼‰
        if it == 1 or (is_log_iter and it % (args.log_every * 5) == 0):
            from bf.menu import visualize_menu
            visualize_menu(flow, menu, t_grid, max_items=8, device=device)
        loss = revenue_loss(flow, batch, menu, t_grid, lam=lam, verbose=verbose, debug=is_log_iter, 
                           v0_test=v0_for_debug, use_gumbel=args.use_gumbel, tau=tau)

        opt.zero_grad()
        loss.backward()
        if verbose:
            print(f"  Backward pass complete. Clipping gradients and updating parameters...", flush=True)
        if args.grad_clip and args.grad_clip > 0:
            # ç¾åœ¨æœ€é©åŒ–ä¸­ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¯¾ã—ã¦clip
            current_params = opt.param_groups[0]['params']
            nn.utils.clip_grad_norm_(current_params, args.grad_clip)
        opt.step()
        if verbose:
            print(f"  Iteration {it} complete!\n", flush=True)

        ema = loss.item() if ema is None else 0.9 * ema + 0.1 * loss.item()
        
        # ç°¡æ˜“é€²æ—ï¼ˆæ¯10 iterationã”ã¨ï¼‰
        if it % 10 == 0 and it % args.log_every != 0:
            print(f"  [{it}/{args.iters}] ...", end='\r', flush=True)
        
        # è©³ç´°ãƒ­ã‚°
        if it % args.log_every == 0:
            dt = time.time() - t0
            iter_per_sec = it / dt if dt > 0 else 0
            eta_sec = (args.iters - it) / iter_per_sec if iter_per_sec > 0 else 0
            eta_min = eta_sec / 60
            temp_str = f"tau={tau:.4f}" if args.use_gumbel else f"lam={lam:.4f}"
            print(f"[{it}/{args.iters}] LRev={loss.item():.6f} ema={ema:.6f} {temp_str} time={dt:.1f}s speed={iter_per_sec:.2f}it/s ETA={eta_min:.1f}min", flush=True)
        
        # å¼±ã„å†åˆæœŸåŒ–ï¼ˆæœªä½¿ç”¨è¦ç´ ã®æ¢ç´¢ç¶­æŒï¼‰
        if args.reinit_every > 0 and it % args.reinit_every == 0:
            # ç¾åœ¨ã®ãƒãƒƒãƒã§é¸æŠç¢ºç‡ã‚’è¨ˆç®—
            with torch.no_grad():
                U_reinit = utilities_matrix(flow, batch, menu, t_grid)
                Z_reinit = torch.softmax(lam * U_reinit, dim=1)
                n_reinit = reinit_unused_elements(flow, menu, Z_reinit, t_grid, 
                                                  threshold=args.reinit_threshold)

        # é€”ä¸­ä¿å­˜
        if args.ckpt_every > 0 and it % args.ckpt_every == 0:
            os.makedirs(args.out_dir, exist_ok=True)
            torch.save({
                "menu": [ (e.beta_raw.detach().cpu(), e.logits.detach().cpu(), e.mus.detach().cpu()) for e in menu ],
                "iteration": it,
                "m": args.m, "K": args.K, "D": args.D,
                "flow_ckpt": args.flow_ckpt,
                "optimizer_state": opt.state_dict(),
                "ema": ema
            }, os.path.join(args.out_dir, f"menu_stage2_step{it}.pt"))
            print(f"Saved checkpoint: menu_stage2_step{it}.pt")

    # æœ€çµ‚ä¿å­˜
    os.makedirs(args.out_dir, exist_ok=True)
    final_path = os.path.join(args.out_dir, "menu_stage2_final.pt")
    torch.save({
        "menu": [ (e.beta_raw.detach().cpu(), e.logits.detach().cpu(), e.mus.detach().cpu()) for e in menu ],
        "iteration": args.iters,
        "m": args.m, "K": args.K, "D": args.D,
        "flow_ckpt": args.flow_ckpt,
        "optimizer_state": opt.state_dict(),
        "ema": ema
    }, final_path)
    print(f"Saved final: {final_path}")

    # ï¼ˆä»»æ„ï¼‰ãƒ†ã‚¹ãƒˆåç›Šï¼ˆãƒãƒ¼ãƒ‰argmaxï¼›æœ¬æ–‡ã§ã®æ¨è«–æ‰‹é †ï¼‰ :contentReference[oaicite:10]{index=10}
    rev = eval_hard_revenue(flow, test[:args.eval_n], menu, t_grid)
    print(f"[TEST] hard-argmax revenue on {min(args.eval_n, len(test))} vals = {rev:.4f}")
    
    # æœ€çµ‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼å¯è¦–åŒ–
    print(f"\n[FINAL] Final menu visualization:", flush=True)
    from bf.menu import visualize_menu
    visualize_menu(flow, menu, t_grid, max_items=10, device=device)

# ---------- è©•ä¾¡ï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã¯ãƒãƒ¼ãƒ‰argmaxï¼›Sec.3.3ï¼‰ ----------
@torch.no_grad()
def eval_hard_revenue(flow: FlowModel, V: List[XORValuation], menu: List[MenuElement], t_grid: torch.Tensor) -> float:
    device = t_grid.device
    total = 0.0
    for v in V:
        U = utilities_matrix(flow, [v], menu, t_grid)[0]   # (K+1,)
        k = int(torch.argmax(U).item())
        u_star = float(U[k].item())
        beta_k = float(menu[k].beta.item())
        total += beta_k if u_star >= 0.0 else 0.0
    return total / max(1, len(V))

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--flow_ckpt", type=str, default="checkpoints/flow_stage1_final.pt")
    ap.add_argument("--resume_ckpt", type=str, default="", help="Resume from Stage2 checkpoint")
    ap.add_argument("--m", type=int, default=50)
    ap.add_argument("--K", type=int, default=512)      # å®Ÿé¨“æ—¢å®šã¯ mâ‰¤100ã§5k/ãã‚Œä»¥ä¸Šã§20kã€‚
    ap.add_argument("--D", type=int, default=8)        # æœ‰é™æ”¯æŒã‚µã‚¤ã‚ºï¼ˆDiracæ··åˆã®å€‹æ•°ï¼‰ã€‚
    ap.add_argument("--iters", type=int, default=20000)
    ap.add_argument("--batch", type=int, default=128)
    ap.add_argument("--lr", type=float, default=3e-1)  # Setup: 0.3 ã‚’æ¨å¥¨ã€‚
    ap.add_argument("--lam_start", type=float, default=1e-3)
    ap.add_argument("--lam_end", type=float, default=1e-1)  # 0.2 â†’ 0.1ï¼ˆé€Ÿã‚ã«ä¸Šã’ã‚‹ï¼‰
    ap.add_argument("--ode_steps", type=int, default=25)
    ap.add_argument("--grad_clip", type=float, default=1.0)
    ap.add_argument("--out_dir", type=str, default="checkpoints")
    ap.add_argument("--ckpt_every", type=int, default=5000)
    ap.add_argument("--log_every", type=int, default=200)
    ap.add_argument("--seed", type=int, default=123)

    # ãƒ‡ãƒ¼ã‚¿ï¼ˆCATS or åˆæˆï¼‰
    ap.add_argument("--cats_glob", type=str, default="")  # ä¾‹: "cats_out/*.txt"
    ap.add_argument("--max_files", type=int, default=None)
    ap.add_argument("--n_val", type=int, default=5000)    # åˆæˆã®æœ¬æ•°
    ap.add_argument("--a", type=int, default=20)          # åˆæˆXORã®åŸå­æ•°ï¼ˆTable 4 ã‚’æ¨¡å€£ï¼‰
    ap.add_argument("--atom_size_mode", type=str, default="small", 
                    choices=["small", "medium", "large", "uniform_3_8"],
                    help="Atom size distribution: small(~5 items), medium(~10), large(~25), uniform_3_8(3-8)")
    ap.add_argument("--eval_n", type=int, default=1000)
    ap.add_argument("--cpu", action="store_true")
    ap.add_argument("--auto_optimize", action="store_true", default=True, help="Auto-optimize parameters for detected hardware")
    ap.add_argument("--no_auto_optimize", action="store_true", help="Disable auto-optimization")
    
    # Gumbel-Softmax + STEï¼ˆStage 2ã®ç ´ç¶»ã‚’ä¿®æ­£ï¼‰
    ap.add_argument("--use_gumbel", action="store_true", help="Use Gumbel-Softmax + STE (fixes training-test gap)")
    ap.add_argument("--tau_start", type=float, default=1.0, help="Initial Gumbel-Softmax temperature")
    ap.add_argument("--tau_end", type=float, default=0.01, help="Final Gumbel-Softmax temperature")
    
    # Î¼ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆï¼‹å†åˆæœŸåŒ–
    ap.add_argument("--warmstart", action="store_true", help="Enable Î¼ warmstart from representative bundles")
    ap.add_argument("--warmstart_grid", type=int, default=200, help="Number of random Î¼ samples for warmstart")
    ap.add_argument("--reinit_every", type=int, default=2000, help="Reinitialize unused elements every N steps (0=disable)")
    ap.add_argument("--reinit_threshold", type=float, default=0.01, help="Reinit elements with z_mean < threshold")
    ap.add_argument("--freeze_beta_iters", type=int, default=1000, help="Freeze Î² for first N iterations (warmup)")

    args = ap.parse_args()
    train_stage2(args)
