# bf/menu.py
import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import List

# v ã¯ XOR è©•ä¾¡å™¨ãªã©ã§ã€v.value(s_bool: Tensor[m]) -> float ã‚’æƒ³å®šï¼ˆæ¨è«–æ™‚ã¯å³å¯†è¨ˆç®—ï¼‰.

class MenuElement(nn.Module):
    def __init__(self, m: int, D: int):
        super().__init__()
        # Î²ã®åˆæœŸåŒ–ã‚’å¤šæ§˜åŒ–ï¼ˆ-3.0ã‹ã‚‰-1.0ã®ç¯„å›²ã§ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        beta_init = torch.randn(1) * 0.5 - 2.0  # mean=-2.0, std=0.5
        self.beta_raw = nn.Parameter(beta_init)
        self.logits = nn.Parameter(torch.zeros(D))        # æ··åˆé‡ã¿ã®ãƒ­ã‚¸ãƒƒãƒˆ
        self.mus = nn.Parameter(torch.zeros(D, m))        # åˆæœŸåˆ†å¸ƒã®æ”¯æŒ Î¼_d^(k)

    @property
    def beta(self) -> torch.Tensor:
        # softplusã§ Î² â‰¥ 0 ã‚’ä¿è¨¼ï¼ˆè«–æ–‡ã® p â‰¥ 0 ã¨æ•´åˆï¼‰
        # log_densityã‚¯ãƒªãƒƒãƒ—å¾Œã¯ã€IRåˆ¶ç´„ãŒè‡ªç„¶ã«Î²ã‚’åˆ¶é™ã™ã‚‹ãŸã‚ã€
        # ä¸Šé™ã¯å¿µã®ãŸã‚ã®å®‰å…¨è£…ç½®ã¨ã—ã¦ç·©ãè¨­å®šï¼ˆ10.0ï¼‰
        beta_unbounded = torch.nn.functional.softplus(self.beta_raw)
        return torch.clamp(beta_unbounded, 0.0, 10.0)  # ç·©ã„ä¸Šé™
    
    @property
    def weights(self) -> torch.Tensor:
        return torch.softmax(self.logits, dim=0)       # simplex

# IRã®ãŸã‚ã«Null Menuã‚’ç”¨æ„ã—ã¦ã„ã‚‹ã€‚
def make_null_element(m: int) -> MenuElement:
    elem = MenuElement(m, D=1)
    with torch.no_grad():
        elem.beta_raw.fill_(float('-inf'))  # softplus(-inf) â‰ˆ 0.0
        elem.logits.fill_(0.0)
        elem.mus.zero_()
    for p in elem.parameters():
        p.requires_grad_(False)
    return elem

# ---- Stage 2: åŠ¹ç”¨ãƒ»æå¤± ------------------------------------------------

# ãƒ¡ãƒ‹ãƒ¥ãƒ¼lã«é–¢ã—ã¦ã€åŠ¹ç”¨ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚
# u^(k)(v) = Î£_d v(s(Î¼_d^(k))) Â· w_d^(k) Â· exp{-Tr[Q(Î¼_d^(k))]âˆ«Î·} âˆ’ Î²^(k)  ï¼ˆEq.(21)ï¼‰
def utility_element(flow, v, elem: MenuElement, t_grid: torch.Tensor) -> torch.Tensor:
    w = elem.weights                             # (D,)
    sT = flow.flow_forward(elem.mus, t_grid)     # (D,m)  ï¼ˆEq.(20)ï¼‰
    s = flow.round_to_bundle(sT)                 # (D,m)  ï¼ˆEq.(21)ã® s(Î¼)ï¼‰
    with torch.no_grad():
        vals = torch.tensor([float(v.value(s[d])) for d in range(s.shape[0])],
                            device=s.device, dtype=s.dtype)  # v(s)
    
    # log-sum-exp ãƒˆãƒªãƒƒã‚¯ã§æ•°å€¤å®‰å®šåŒ–ï¼ˆå¼(21)ã®æŒ‡æ•°é‡ã¿ï¼‰
    log_w = torch.log(w + 1e-10)                              # log(w_d)
    log_density = flow.log_density_weight(elem.mus, t_grid)   # (D,) = -Tr[Q]âˆ«Î·
    log_weights = log_w + log_density                         # (D,) = log(w_d) + log_density
    
    # log-sum-exp: u = exp(M) * Î£_d exp(log_w_d - M) * v_d - Î²
    M = torch.max(log_weights)
    weighted_sum = (torch.exp(log_weights - M) * vals).sum()
    u = torch.exp(M) * weighted_sum - elem.beta
    return u

# ãƒãƒƒãƒå‡¦ç†ç‰ˆï¼šå…¨menu elementsã‚’ä¸€åº¦ã«å‡¦ç†
# U[b,k] = u^(k)(v_b)  ï¼ˆEq.(21) ã‚’å…¨çµ„ã§ï¼‰
def utilities_matrix_batched(flow, V: List, menu: List[MenuElement], t_grid: torch.Tensor, verbose: bool = False, debug: bool = False, v0_test=None) -> torch.Tensor:
    """
    é«˜é€Ÿãƒãƒƒãƒå‡¦ç†ç‰ˆï¼šå…¨menu elementsã®musã‚’çµ±åˆã—ã¦ä¸€åº¦ã«å‡¦ç†
    é€Ÿåº¦: O(B * K * D) â†’ O(B * 1)ã®flow_forwardå‘¼ã³å‡ºã—
    """
    K = len(menu)
    B = len(V)
    device = t_grid.device
    
    if verbose:
        print(f"  [Batched] Collecting all mus from {K} menu elements...", flush=True)
    
    # nullè¦ç´ ï¼ˆæœ€å¾Œã®è¦ç´ ã€D=1ï¼‰ã‚’åˆ†é›¢
    menu_main = menu[:-1]  # é€šå¸¸ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ï¼ˆD=8ãªã©ï¼‰
    null_elem = menu[-1]   # nullè¦ç´ ï¼ˆD=1ï¼‰
    K_main = len(menu_main)
    
    if verbose:
        print(f"  [Batched] K={K}, K_main={K_main}, menu length={len(menu)}", flush=True)
        print(f"  [Batched] Splitting: menu_main has {K_main} elements, null is 1 element", flush=True)
    
    # é€šå¸¸ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã®musã‚’çµ±åˆ: [(D, m)] * K_main -> (K_main, D, m)
    all_mus = torch.stack([elem.mus for elem in menu_main])  # (K_main, D, m)
    all_weights = torch.stack([elem.weights for elem in menu_main])  # (K_main, D)
    all_betas = torch.stack([elem.beta for elem in menu_main]).squeeze()  # (K_main,)
    
    K_main, D, m = all_mus.shape
    
    if verbose:
        print(f"  [Batched] Running flow_forward on {K_main*D} bundles at once...", flush=True)
        # Î¼ã®ç¯„å›²ã‚’ç¢ºèª
        print(f"  [Batched] all_mus range: [{all_mus.min().item():.4f}, {all_mus.max().item():.4f}], mean={all_mus.mean().item():.4f}", flush=True)
    
    # å…¨ã¦ã®Î¼ã‚’ãƒãƒƒãƒå‡¦ç†: (K_main, D, m) -> (K_main*D, m)
    mus_flat = all_mus.view(K_main * D, m)
    sT_flat = flow.flow_forward(mus_flat, t_grid)  # (K_main*D, m) - ä¸€åº¦ã®å‘¼ã³å‡ºã—ï¼
    
    if verbose:
        print(f"  [Batched] sT_flat range BEFORE rounding: [{sT_flat.min().item():.4f}, {sT_flat.max().item():.4f}], mean={sT_flat.mean().item():.4f}", flush=True)
    
    s_flat = flow.round_to_bundle(sT_flat)  # (K_main*D, m)
    
    if verbose:
        print(f"  [Batched] s_flat range AFTER rounding: [{s_flat.min().item():.4f}, {s_flat.max().item():.4f}], mean={s_flat.mean().item():.4f}", flush=True)
    
    # log_density_weightã‚‚ãƒãƒƒãƒå‡¦ç†
    log_density_flat = flow.log_density_weight(mus_flat, t_grid)  # (K_main*D,)
    # å®‰å®šåŒ–ï¼šlog_densityã‚’ã‚¯ãƒªãƒƒãƒ—ï¼ˆexpçˆ†ç™ºã‚’é˜²æ­¢ï¼‰
    log_density_flat = torch.clamp(log_density_flat, -10.0, 0.0)  # å¯†åº¦é‡ã¿ â‰¤ 1.0
    log_density = log_density_flat.view(K_main, D)  # (K_main, D)
    
    # nullè¦ç´ ã‚‚ä¸€åº¦ã«å‡¦ç†
    null_sT = flow.flow_forward(null_elem.mus, t_grid)  # (1, m)
    null_s = flow.round_to_bundle(null_sT)  # (1, m)
    null_log_density = flow.log_density_weight(null_elem.mus, t_grid)  # (1,)
    null_log_density = torch.clamp(null_log_density, -10.0, 0.0)  # å®‰å®šåŒ–
    null_weight = null_elem.weights  # (1,)
    null_beta = null_elem.beta  # scalar
    
    # å„valuationã«ã¤ã„ã¦åŠ¹ç”¨ã‚’è¨ˆç®—
    # æ³¨æ„ï¼šmenuã¯ K_main + 1 å€‹ï¼ˆnullå«ã‚€ï¼‰
    U = torch.zeros(B, K, device=device, dtype=torch.float32)  # K = K_main + 1
    
    # s_flatã‚’CPUã«ä¸€åº¦ã ã‘è»¢é€ï¼ˆå…¨valuationã§å…±æœ‰ï¼‰
    s_flat_cpu = s_flat.cpu()
    
    for i, v in enumerate(V):
        if verbose and i % 10 == 0:
            print(f"  [Batched] Processing valuation {i+1}/{B}...", flush=True)
        
        # å…¨bundlesã®ä¾¡å€¤ã‚’ä¸€åº¦ã«è¨ˆç®—
        if hasattr(v, 'batch_value'):
            vals_flat = v.batch_value(s_flat_cpu)  # (K_main*D,)
            
            # ãƒ‡ãƒãƒƒã‚°ï¼šè¤‡æ•°ã®valuationã‚’ãƒã‚§ãƒƒã‚¯
            if debug and i < 3:  # æœ€åˆã®3å€‹ã®valuationã‚’ãƒã‚§ãƒƒã‚¯
                non_zero_count = (vals_flat > 0).sum().item()
                print(f"  [DEBUG] Valuation {i}: batch_value min={vals_flat.min().item():.4f}, max={vals_flat.max().item():.4f}, non-zero={non_zero_count}/{len(vals_flat)}", flush=True)
                print(f"  [DEBUG] Valuation {i}: num_atoms={len(v.atoms)}, max_price={max(p for _, p in v.atoms):.4f}", flush=True)
            
            vals_flat = vals_flat.to(device)  # GPUã«æˆ»ã™
        else:
            # fallback: é€æ¬¡å‡¦ç†
            vals_flat = torch.tensor([v.value(s_flat_cpu[j]) for j in range(K_main*D)],
                                    device=device, dtype=torch.float32)
        
        vals = vals_flat.view(K_main, D)  # (K_main, D)
        
        # å„menu elementã®åŠ¹ç”¨ã‚’è¨ˆç®—ï¼ˆlog-sum-expï¼‰
        log_w = torch.log(all_weights + 1e-10)  # (K_main, D)
        log_weights = log_w + log_density  # (K_main, D)
        
        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®valuationã§è©³ç´°ã‚’å‡ºåŠ›ï¼ˆdebugãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ã‚‹ã¨ãï¼‰
        if debug and i == 0:
            # bundleã®ä¸­èº«ã‚’ç¢ºèª
            print(f"  [DEBUG] s_flat shape: {s_flat.shape}, unique values: {torch.unique(s_flat).tolist()}", flush=True)
            
            # bundleã®å¤šæ§˜æ€§ã‚’ç¢ºèª
            unique_bundles = torch.unique(s_flat, dim=0)
            print(f"  [DEBUG] Number of UNIQUE bundles: {len(unique_bundles)} out of {len(s_flat)}", flush=True)
            
            # å„bundleã®ãƒ“ãƒƒãƒˆæ•°ï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•°ï¼‰ã®åˆ†å¸ƒ
            num_items = s_flat.sum(dim=1)  # å„bundleã®ã‚¢ã‚¤ãƒ†ãƒ æ•°
            print(f"  [DEBUG] Items per bundle: min={num_items.min().item():.0f}, max={num_items.max().item():.0f}, mean={num_items.mean().item():.1f}, std={num_items.std().item():.1f}", flush=True)
            
            print(f"  [DEBUG] s_flat[0:5]: {s_flat[0:5].tolist()}", flush=True)
            print(f"  [DEBUG] Number of valuation atoms: {len(v.atoms)}", flush=True)
            if len(v.atoms) > 0:
                print(f"  [DEBUG] First atom: mask={v.atoms[0][0]}, price={v.atoms[0][1]:.4f}", flush=True)
                # ã“ã®valuationã®æœ€å¤§å¯èƒ½ä¾¡å€¤ï¼ˆæœ€ã‚‚é«˜ã„atomã®ä¾¡æ ¼ï¼‰
                max_possible_value = max(price for _, price in v.atoms)
                print(f"  [DEBUG] Max possible value from this valuation: {max_possible_value:.4f}", flush=True)
            else:
                print(f"  [DEBUG] WARNING: This valuation has NO atoms!", flush=True)
            
            # value()ãƒ¡ã‚½ãƒƒãƒ‰ã§ç›´æ¥ãƒ†ã‚¹ãƒˆï¼ˆCPUã«è»¢é€ã—ã¦ã‹ã‚‰ï¼‰
            print(f"  [DEBUG] Calling v.value(s_flat_cpu[0]) with debug=True:", flush=True)
            test_val_direct = v.value(s_flat_cpu[0], debug=True)
            print(f"  [DEBUG] v.value(s_flat_cpu[0]) = {test_val_direct:.4f}", flush=True)
            
            # èµ·å‹•æ™‚ã«æˆåŠŸã—ãŸv0ã§ã‚‚ãƒ†ã‚¹ãƒˆ
            if v0_test is not None:
                print(f"  [DEBUG] Testing with v0 (startup valuation) that worked before:", flush=True)
                v0_test_result = v0_test.value(s_flat_cpu[0])
                print(f"  [DEBUG] v0.value(s_flat_cpu[0]) = {v0_test_result:.4f}", flush=True)
                all_items_cpu = torch.ones(50, device='cpu')
                v0_all_items = v0_test.value(all_items_cpu)
                print(f"  [DEBUG] v0.value(all_items) = {v0_all_items:.4f} (should be 0.9496)", flush=True)
            
            # ãƒã‚¹ã‚¯ã®è©³ç´°ã‚’ç¢ºèª
            from bf.valuation import _tensor_to_mask
            test_mask = _tensor_to_mask(s_flat_cpu[0])
            print(f"  [DEBUG] _tensor_to_mask(s_flat_cpu[0]) = {test_mask}", flush=True)
            
            # æ¯”è¼ƒï¼šGPU tensorã§è©¦ã™ã¨ã©ã†ãªã‚‹ã‹
            try:
                test_mask_gpu = _tensor_to_mask(s_flat[0])
                print(f"  [DEBUG] GPU tensor mask: {test_mask_gpu}, CPU tensor mask: {test_mask}, equal? {test_mask_gpu == test_mask}", flush=True)
            except Exception as e:
                print(f"  [DEBUG] GPU tensor failed: {e}", flush=True)
            print(f"  [DEBUG] bin(test_mask) = {bin(test_mask)}", flush=True)
            print(f"  [DEBUG] First atom mask = {v.atoms[0][0]}, bin = {bin(v.atoms[0][0])}", flush=True)
            
            # ãƒ“ãƒƒãƒˆä½ç½®ã‚’ç›´æ¥æ¯”è¼ƒ
            bundle_bits = [i for i in range(50) if (test_mask & (1 << i)) != 0]
            atom0_bits = [i for i in range(50) if (v.atoms[0][0] & (1 << i)) != 0]
            print(f"  [DEBUG] Bundle[0] items (bit positions): {bundle_bits}", flush=True)
            print(f"  [DEBUG] Atom[0] items (bit positions): {atom0_bits}", flush=True)
            
            # ã©ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒAtomã«ã‚ã‚‹ãŒBundleã«ãªã„ï¼Ÿ
            missing_in_bundle = [i for i in atom0_bits if i not in bundle_bits]
            print(f"  [DEBUG] Items in Atom[0] but NOT in Bundle[0]: {missing_in_bundle}", flush=True)
            
            # é€†ï¼šBundleã«ã‚ã‚‹ãŒatomã«ãªã„
            extra_in_bundle = [i for i in bundle_bits if i not in atom0_bits]
            print(f"  [DEBUG] Items in Bundle[0] but NOT in Atom[0]: {extra_in_bundle} (count: {len(extra_in_bundle)})", flush=True)
            
            # æ‰‹å‹•ã§TâŠ†Sã‚’ãƒã‚§ãƒƒã‚¯
            atom_mask = v.atoms[0][0]
            test_subset = (atom_mask & (~test_mask)) == 0
            print(f"  [DEBUG] Is atom[0] âŠ† s_flat[0]? {test_subset} (atom_mask & ~test_mask = {atom_mask & (~test_mask)})", flush=True)
            
            # å…¨ã¦ã®atomã‚’ãƒã‚§ãƒƒã‚¯
            matching_atoms = []
            print(f"  [DEBUG] Checking all {len(v.atoms)} atoms...", flush=True)
            for idx, (atom_mask, price) in enumerate(v.atoms[:5]):  # æœ€åˆã®5å€‹ã ã‘
                check_result = (atom_mask & (~test_mask))
                is_subset = check_result == 0
                print(f"  [DEBUG]   Atom {idx}: mask={atom_mask}, price={price:.4f}, atom&~test={check_result}, is_subset={is_subset}", flush=True)
                if is_subset:
                    matching_atoms.append((atom_mask, price))
            
            # å…¨atomã‚’ãƒã‚§ãƒƒã‚¯
            for atom_mask, price in v.atoms:
                if (atom_mask & (~test_mask)) == 0:
                    matching_atoms.append((atom_mask, price))
            
            print(f"  [DEBUG] Number of matching atoms: {len(matching_atoms)}", flush=True)
            if len(matching_atoms) > 0:
                print(f"  [DEBUG] Max matching price: {max(p for _, p in matching_atoms):.4f}", flush=True)
            
            # ã“ã®valuationã§å…¨ONã®bundleã‚’è©•ä¾¡ï¼ˆèµ·å‹•æ™‚ãƒ†ã‚¹ãƒˆã¨åŒã˜ï¼‰
            all_items_cpu = torch.ones(50, device='cpu')
            all_items_val = v.value(all_items_cpu)
            print(f"  [DEBUG] THIS valuation's value for ALL items: {all_items_val:.4f}", flush=True)
            
            # é€†ã«ã€test_maskãŒã©ã®atomã®ã‚¹ãƒ¼ãƒ‘ãƒ¼ã‚»ãƒƒãƒˆã‹ç¢ºèª
            print(f"  [DEBUG] Checking if test_mask is a superset of any atom...", flush=True)
            for idx, (atom_mask, price) in enumerate(v.atoms[:3]):
                # TâŠ†Sã®åˆ¤å®šãŒæ­£ã—ã„ã‹ç¢ºèª
                subset_check = (atom_mask & (~test_mask)) == 0
                print(f"  [DEBUG]   Atom {idx}: TâŠ†S? {subset_check}", flush=True)
                # ãƒ“ãƒƒãƒˆæ•°ã‚’ç¢ºèª
                atom_bits = bin(atom_mask).count('1')
                test_bits = bin(test_mask).count('1')
                print(f"  [DEBUG]     Atom has {atom_bits} bits set, test has {test_bits} bits set", flush=True)
                
                # atomã®æœ€ä¸Šä½ãƒ“ãƒƒãƒˆã‚’ç¢ºèª
                highest_bit = atom_mask.bit_length() - 1  # æœ€ä¸Šä½ãƒ“ãƒƒãƒˆã®ä½ç½®
                print(f"  [DEBUG]     Highest bit position: {highest_bit} (should be < 50)", flush=True)
            
            # batch_value()ã®çµæœ
            print(f"  [DEBUG] vals range: [{vals.min().item():.4f}, {vals.max().item():.4f}]", flush=True)
            print(f"  [DEBUG] vals[0:5]: {vals[0, :].tolist()}", flush=True)
            
            # vals_flatã®æœ€åˆã®10å€‹
            print(f"  [DEBUG] vals_flat[0:10]: {vals_flat[0:10].tolist()}", flush=True)
            
            print(f"  [DEBUG] log_w range: [{log_w.min().item():.4f}, {log_w.max().item():.4f}]", flush=True)
            print(f"  [DEBUG] log_density range: [{log_density.min().item():.4f}, {log_density.max().item():.4f}]", flush=True)
            print(f"  [DEBUG] log_weights range: [{log_weights.min().item():.4f}, {log_weights.max().item():.4f}]", flush=True)
        
        # log-sum-exp per menu element
        M = torch.max(log_weights, dim=1, keepdim=True)[0]  # (K_main, 1)
        weighted_sum = (torch.exp(log_weights - M) * vals).sum(dim=1)  # (K_main,)
        u_main = torch.exp(M.squeeze(1)) * weighted_sum - all_betas  # (K_main,)
        
        if debug and i == 0:
            print(f"  [DEBUG] M range: [{M.min().item():.4f}, {M.max().item():.4f}]", flush=True)
            print(f"  [DEBUG] exp(M) range: [{torch.exp(M).min().item():.4e}, {torch.exp(M).max().item():.4e}]", flush=True)
            print(f"  [DEBUG] weighted_sum range: [{weighted_sum.min().item():.4f}, {weighted_sum.max().item():.4f}]", flush=True)
            print(f"  [DEBUG] u_main range: [{u_main.min().item():.4f}, {u_main.max().item():.4f}]", flush=True)
            print(f"  [DEBUG] all_betas range: [{all_betas.min().item():.4f}, {all_betas.max().item():.4f}]", flush=True)
            # Î²ã®åˆ†å¸ƒã‚’ç¢ºèª
            unique_betas = torch.unique(all_betas)
            print(f"  [DEBUG] Number of unique beta values: {len(unique_betas)}, values: {unique_betas.tolist()}", flush=True)
        
        # nullè¦ç´ ã®åŠ¹ç”¨ã‚’è¨ˆç®—ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        if hasattr(v, 'value'):
            null_val = v.value(null_s[0])
        else:
            null_val = 0.0
        log_w_null = torch.log(null_weight + 1e-10)
        log_weight_null = log_w_null + null_log_density
        u_null = torch.exp(log_weight_null[0]) * null_val - null_beta
        
        # çµåˆ: [u_main, u_null]
        if debug and i == 0:
            print(f"  [DEBUG] u_main shape: {u_main.shape}, u_null shape: {u_null.shape if isinstance(u_null, torch.Tensor) else 'scalar'}", flush=True)
            print(f"  [DEBUG] U shape: {U.shape}, U[i, :-1] will be shape {U[i, :-1].shape}", flush=True)
            print(f"  [DEBUG] Assigning u_main (len={len(u_main)}) to U[{i}, :-1] (len={len(U[i, :-1])})", flush=True)
        
        U[i, :-1] = u_main
        U[i, -1] = u_null
        
        if debug and i == 0:
            print(f"  [DEBUG] After assignment: U[{i}] range = [{U[i].min().item():.4f}, {U[i].max().item():.4f}]", flush=True)
            print(f"  [DEBUG] U[{i}, -1] (null) = {U[i, -1].item():.4f}", flush=True)
    
    if verbose:
        print(f"  [Batched] Utilities computed: {U.shape}", flush=True)
    
    return U  # (B, K)

# ãƒãƒªãƒ¥ãƒ¼é›†åˆVã¨ã€å…¨ã¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼lã«é–¢ã—ã¦ã€åŠ¹ç”¨ã‚’è¡Œåˆ—è¡¨ç¤ºã—ã¦ã„ã‚‹ã€‚
# U[b,k] = u^(k)(v_b)  ï¼ˆEq.(21) ã‚’å…¨çµ„ã§ï¼‰
def utilities_matrix(flow, V: List, menu: List[MenuElement], t_grid: torch.Tensor, verbose: bool = False, debug: bool = False, v0_test=None) -> torch.Tensor:
    # ãƒãƒƒãƒå‡¦ç†ç‰ˆã‚’ä½¿ç”¨
    return utilities_matrix_batched(flow, V, menu, t_grid, verbose=verbose, debug=debug, v0_test=v0_test)

# å­¦ç¿’æ™‚ã®é€£ç¶šå‰²ã‚Šå½“ã¦ã‚’è¿”ã™ã€‚
# z^(k)(v) = SoftMax_k( Î» Â· u^(k)(v) )  ï¼ˆEq.(23)ï¼‰
def soft_assignment(U: torch.Tensor, lam: float) -> torch.Tensor:
    return torch.softmax(lam * U, dim=1)

# æœŸå¾…æç›Šã®è² ã®å€¤ã‚’æœ€å°åŒ–ã—ã¦ã„ã‚‹ã€‚
def revenue_loss(flow, V: List, menu: List[MenuElement], t_grid: torch.Tensor, lam: float = 0.1, 
                 verbose: bool = False, debug: bool = False, v0_test=None,
                 use_gumbel: bool = False, tau: float = 0.1) -> torch.Tensor:
    """
    Stage 2ã®åç›Šæå¤±é–¢æ•°
    
    Args:
        flow: FlowModel
        V: Valuations
        menu: List[MenuElement]
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        lam: Softmaxæ¸©åº¦ï¼ˆuse_gumbel=Falseã®æ™‚ã®ã¿ä½¿ç”¨ï¼‰
        verbose: è©³ç´°ãƒ­ã‚°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        v0_test: ãƒ†ã‚¹ãƒˆç”¨valuation
        use_gumbel: Trueãªã‚‰Gumbel-Softmax + STEã€Falseãªã‚‰Softmax relaxation
        tau: Gumbel-Softmaxæ¸©åº¦ï¼ˆuse_gumbel=Trueã®æ™‚ã®ã¿ä½¿ç”¨ï¼‰
        
    Returns:
        -revenue (æå¤±)
    """
    # LRev = -(1/|V|) Î£_v Î£_k z_k(v) Î²_k  ï¼ˆEq.(22)ï¼‰
    if verbose:
        print(f"  Computing utilities matrix ({len(V)} valuations Ã— {len(menu)} menu elements)...", flush=True)
    U = utilities_matrix(flow, V, menu, t_grid, verbose=verbose, debug=debug, v0_test=v0_test)  # (B,K)
    beta = torch.stack([elem.beta for elem in menu]).squeeze()     # (K,)
    
    if use_gumbel:
        # Gumbel-Softmax + STE: Forwardæ™‚ã«hard argmaxã€Backwardæ™‚ã«softå‹¾é…
        if verbose:
            print(f"  Computing Gumbel-Softmax assignment (tau={tau}, hard=True)...", flush=True)
        
        from bf.utils import gumbel_softmax
        if debug:
            print(f"  [DEBUG-GUMBEL] U shape: {U.shape}, U range: [{U.min().item():.4f}, {U.max().item():.4f}]", flush=True)
        Z = gumbel_softmax(U, tau=tau, hard=True, dim=1)  # (B, K) one-hot
        if debug:
            print(f"  [DEBUG-GUMBEL] Z shape: {Z.shape}, Z sum per row: {Z.sum(dim=1)[:5].tolist()}", flush=True)
        
        # IRåˆ¶ç´„ã®æ˜ç¤ºçš„é©ç”¨: é¸æŠã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®åŠ¹ç”¨ãŒè² ãªã‚‰åç›Šã‚¼ãƒ­
        selected_idx = torch.argmax(Z, dim=1)  # (B,)
        selected_utility = torch.gather(U, 1, selected_idx.unsqueeze(1)).squeeze(1)  # (B,)
        ir_mask = (selected_utility >= 0.0).float()  # (B,)
        
        # åç›Šè¨ˆç®—: IRåˆ¶ç´„ã‚’æº€ãŸã•ãªã„å ´åˆã¯0
        if debug:
            print(f"  [DEBUG-GUMBEL] beta shape: {beta.shape}, beta range: [{beta.min().item():.4f}, {beta.max().item():.4f}]", flush=True)
        rev_per_buyer = (Z * beta.unsqueeze(0)).sum(dim=1) * ir_mask  # (B,)
        rev = rev_per_buyer.mean()
        
        if debug:
            print(f"  [DEBUG-REV-GUMBEL] U range: [{U.min().item():.4f}, {U.max().item():.4f}]", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] U mean: {U.mean().item():.4f}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] beta range: [{beta.min().item():.4f}, {beta.max().item():.4f}]", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] beta mean: {beta.mean().item():.4f}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Z shape: {Z.shape}, is one-hot: {(Z.sum(dim=1) == 1.0).all().item()}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Selected indices: {selected_idx[:5].tolist()}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Selected utilities: {selected_utility[:5].tolist()}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] IR constraint satisfied: {ir_mask.sum().item()}/{len(ir_mask)}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Revenue per buyer (first 5): {rev_per_buyer[:5].tolist()}", flush=True)
    else:
        # å¾“æ¥ã®Softmax relaxation
        if verbose:
            print(f"  Computing soft assignment (lambda={lam})...", flush=True)
        Z = soft_assignment(U, lam)  # (B,K)
        rev = (Z * beta.unsqueeze(0)).sum(dim=1).mean()
        
        if debug:
            print(f"  [DEBUG-REV] Z shape: {Z.shape}, beta shape: {beta.shape}", flush=True)
            print(f"  [DEBUG-REV] Z[0] (first valuation selection probs): min={Z[0].min().item():.6f}, max={Z[0].max().item():.6f}", flush=True)
            print(f"  [DEBUG-REV] Z[:, -1] (null selection): min={Z[:, -1].min().item():.6f}, max={Z[:, -1].max().item():.6f}, mean={Z[:, -1].mean().item():.6f}", flush=True)
            print(f"  [DEBUG-REV] beta range: [{beta.min().item():.4f}, {beta.max().item():.4f}]", flush=True)
            print(f"  [DEBUG-REV] beta[-1] (null price): {beta[-1].item():.6f}", flush=True)
            print(f"  [DEBUG-REV] (Z * beta) per valuation: {(Z * beta.unsqueeze(0)).sum(dim=1)[:5].tolist()}", flush=True)
    
    if verbose:
        print(f"  Revenue: {rev.item():.6f} ({'Gumbel+STE' if use_gumbel else 'Softmax'})", flush=True)
    return -rev

# ãƒ†ã‚¹ãƒˆæ™‚
@torch.no_grad()
def visualize_menu(flow, menu: List[MenuElement], t_grid: torch.Tensor, 
                   max_items: int = 10, device: torch.device = None) -> None:
    """
    ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®å†…å®¹ã‚’å¯è¦–åŒ–ï¼ˆã©ã®å•†å“ã®çµ„ã¿åˆã‚ã›ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
    """
    if device is None:
        device = next(menu[0].parameters()).device
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ MENU VISUALIZATION (showing first {max_items} items)")
    print(f"{'='*60}")
    
    for k, elem in enumerate(menu[:max_items]):
        # ãƒãƒ³ãƒ‰ãƒ«ç”Ÿæˆï¼ˆÎ¼ã‹ã‚‰ï¼‰
        with torch.no_grad():
            s_T = flow.flow_forward(elem.mus, t_grid)  # (D, m)
            s = flow.round_to_bundle(s_T)  # (D, m) -> discrete bundles
            
        # å„ãƒãƒ³ãƒ‰ãƒ«ã®å†…å®¹ã‚’è¡¨ç¤º
        beta_val = elem.beta.item()
        print(f"\nğŸ½ï¸  Menu Item {k+1}: Price = {beta_val:.4f}")
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ³ãƒ‰ãƒ«ã‚’æŠ½å‡º
        unique_bundles = []
        for d in range(s.shape[0]):
            bundle = s[d].cpu().numpy()
            bundle_tuple = tuple(bundle.astype(int))
            if bundle_tuple not in unique_bundles:
                unique_bundles.append(bundle_tuple)
        
        print(f"   ğŸ“¦ Generated bundles ({len(unique_bundles)} unique):")
        for i, bundle in enumerate(unique_bundles[:5]):  # æœ€å¤§5å€‹è¡¨ç¤º
            items = [j for j, val in enumerate(bundle) if val == 1]
            if items:
                items_str = ", ".join([f"Item_{j}" for j in items])
                print(f"      {i+1}. [{items_str}]")
            else:
                print(f"      {i+1}. [Empty bundle]")
        
        if len(unique_bundles) > 5:
            print(f"      ... and {len(unique_bundles)-5} more bundles")
    
    if len(menu) > max_items:
        print(f"\n... and {len(menu)-max_items} more menu items")
    
    print(f"{'='*60}\n")

# ãƒ†ã‚¹ãƒˆæ™‚ã¯SoftMaxã§ã¯ãªãã€argmaxã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚
def infer_choice(flow, v, menu: List[MenuElement], t_grid: torch.Tensor) -> int:
    # æ¨è«–æ™‚ã¯ argmaxï¼ˆæœ¬æ–‡ Sec.3.3 / Eq.(23) ã®ãƒãƒ¼ãƒ‰ç‰ˆï¼‰
    U = torch.stack([utility_element(flow, v, elem, t_grid) for elem in menu])  # (K,)
    return int(torch.argmax(U).item())
