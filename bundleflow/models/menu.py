# bundleflow/models/menu.py
"""
MenuElement: ä¸€è¦ç´  k ã® (Ï†_k, p_k)
Mechanism: å…¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆå…±é€š v_Î¸ ã¨ Kå€‹ã®(Ï†_k,p_k)ï¼‰

ç›®çš„: å…±é€šã®v_Î¸ã¨å„è¦ç´ (Ï†_k,p_k)ã‹ã‚‰ãƒ¡ãƒ‹ãƒ¥ãƒ¼Mã‚’æ§‹æˆã—æœŸå¾…åå…¥E[R]ã‚’æœ€å¤§åŒ–.
è¨˜å·: K=ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ æ•°. ä¾¡å€¤é–¢æ•°v(b)ã¯å¤–ç”Ÿ.
API: expected_revenue(valuation_batch), argmax_menu(valuation_batch)
"""

import torch
import torch.nn as nn
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

# v ã¯ XOR è©•ä¾¡å™¨ãªã©ã§ã€v.value(s_bool: Tensor[m]) -> float ã‚’æƒ³å®šï¼ˆæ¨è«–æ™‚ã¯å³å¯†è¨ˆç®—ï¼‰.

class MenuElement(nn.Module):
    """
    ä¸€è¦ç´  k ã® (Ï†_k, p_k)ã€‚p_kâ‰¥0ã¯softplusç­‰ã§ä¿è¨¼ã€‚
    
    ç›®çš„: å„ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ kã®åˆæœŸåˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Ï†_kã¨ä¾¡æ ¼p_kã‚’å­¦ç¿’
    è¨˜å·: Ï†_k = {Î¼_d^(k), w_d^(k)}_d, p_k = softplus(Î²_k)
    """
    
    def __init__(self, m: int, D: int):
        super().__init__()
        self.m = m
        self.D = D
        
        # Î²ã®åˆæœŸåŒ–ã‚’å¤šæ§˜åŒ–ï¼ˆ-3.0ã‹ã‚‰-1.0ã®ç¯„å›²ã§ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        beta_init = torch.randn(1) * 0.5 - 2.0  # mean=-2.0, std=0.5
        self.beta_raw = nn.Parameter(beta_init)
        self.logits = nn.Parameter(torch.zeros(D))        # æ··åˆé‡ã¿ã®ãƒ­ã‚¸ãƒƒãƒˆ
        self.mus = nn.Parameter(torch.zeros(D, m))        # åˆæœŸåˆ†å¸ƒã®æ”¯æŒ Î¼_d^(k)

    @property
    def beta(self) -> torch.Tensor:
        """
        ä¾¡æ ¼ p_k = softplus(Î²_k) â‰¥ 0
        
        Returns:
            p_k: ä¾¡æ ¼ (scalar)
        """
        # softplusã§ Î² â‰¥ 0 ã‚’ä¿è¨¼ï¼ˆè«–æ–‡ã® p â‰¥ 0 ã¨æ•´åˆï¼‰
        # log_densityã‚¯ãƒªãƒƒãƒ—å¾Œã¯ã€IRåˆ¶ç´„ãŒè‡ªç„¶ã«Î²ã‚’åˆ¶é™ã™ã‚‹ãŸã‚ã€
        # ä¸Šé™ã¯å¿µã®ãŸã‚ã®å®‰å…¨è£…ç½®ã¨ã—ã¦ç·©ãè¨­å®šï¼ˆ10.0ï¼‰
        beta_unbounded = torch.nn.functional.softplus(self.beta_raw)
        return torch.clamp(beta_unbounded, 0.0, 10.0)  # ç·©ã„ä¸Šé™
    
    @property
    def weights(self) -> torch.Tensor:
        """
        æ··åˆé‡ã¿ w_d^(k)
        
        Returns:
            w: é‡ã¿ (D,) simplex
        """
        return torch.softmax(self.logits, dim=0)       # simplex
    
    def sample_init(self, n: int) -> torch.Tensor:
        """
        åˆæœŸåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« z ~ p_0(Ï†_k)
        
        Args:
            n: ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            z: ã‚µãƒ³ãƒ—ãƒ« (n, m)
        """
        # æ··åˆDiracåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«
        device = self.mus.device
        cat = torch.distributions.Categorical(probs=self.weights)
        idx = cat.sample((n,))  # (n,)
        return self.mus[idx]  # (n, m)
    
    def price(self) -> torch.Tensor:
        """
        ä¾¡æ ¼ p_k
        
        Returns:
            p_k: ä¾¡æ ¼ (scalar)
        """
        return self.beta
    
    def to(self, device):
        """ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•ãƒ¡ã‚½ãƒƒãƒ‰"""
        super().to(device)
        return self


def make_null_element(m: int) -> MenuElement:
    """
    IRã®ãŸã‚ã«Null Menuã‚’ç”¨æ„
    
    Args:
        m: å•†å“æ•°
        
    Returns:
        null_elem: ãƒŒãƒ«è¦ç´ ï¼ˆä¾¡æ ¼0ã€åŠ¹ç”¨0ï¼‰
    """
    elem = MenuElement(m, D=1)
    with torch.no_grad():
        elem.beta_raw.fill_(float('-inf'))  # softplus(-inf) â‰ˆ 0.0
        elem.logits.fill_(0.0)
        elem.mus.zero_()
    for p in elem.parameters():
        p.requires_grad_(False)
    return elem


class Mechanism(nn.Module):
    """
    å…¨ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼ˆå…±é€š v_Î¸ ã¨ Kå€‹ã®(Ï†_k,p_k)ï¼‰
    
    ç›®çš„: å…±é€šã®v_Î¸ã¨å„è¦ç´ (Ï†_k,p_k)ã‹ã‚‰ãƒ¡ãƒ‹ãƒ¥ãƒ¼Mã‚’æ§‹æˆã—æœŸå¾…åå…¥E[R]ã‚’æœ€å¤§åŒ–.
    è¨˜å·: K=ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ æ•°. ä¾¡å€¤é–¢æ•°v(b)ã¯å¤–ç”Ÿ.
    API: expected_revenue(valuation_batch), argmax_menu(valuation_batch)
    """
    
    def __init__(self, flow, menu: List[MenuElement]):
        super().__init__()
        self.flow = flow
        self.menu = menu
        self.K = len(menu)
    
    def expected_revenue(self, valuation_batch) -> torch.Tensor:
        """
        æœŸå¾…åå…¥ã‚’è¨ˆç®—
        
        Args:
            valuation_batch: è©•ä¾¡é–¢æ•°ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            revenue: æœŸå¾…åå…¥
        """
        # åŠ¹ç”¨è¡Œåˆ—ã‚’è¨ˆç®—
        t_grid = torch.linspace(0.0, 1.0, steps=25, device=next(self.menu[0].parameters()).device)
        U = utilities_matrix(self.flow, valuation_batch, self.menu, t_grid)  # (B, K)
        
        # ä¾¡æ ¼ã‚’å–å¾—
        beta = torch.stack([elem.beta for elem in self.menu]).squeeze()  # (K,)
        
        # ã‚½ãƒ•ãƒˆå‰²å½“
        lam = 0.1  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        Z = torch.softmax(lam * U, dim=1)  # (B, K)
        
        # æœŸå¾…åå…¥
        revenue = (Z * beta.unsqueeze(0)).sum(dim=1).mean()
        
        return revenue
    
    def argmax_menu(self, valuation_batch) -> Dict[str, Any]:
        """
        ãƒãƒ¼ãƒ‰å‰²å½“ã§ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠçµæœ
        
        Args:
            valuation_batch: è©•ä¾¡é–¢æ•°ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            result: å‰²å½“/ä¾¡æ ¼/æœŸå¾…åšç”Ÿãªã©ã®è¾æ›¸
        """
        # åŠ¹ç”¨è¡Œåˆ—ã‚’è¨ˆç®—
        t_grid = torch.linspace(0.0, 1.0, steps=25, device=next(self.menu[0].parameters()).device)
        U = utilities_matrix(self.flow, valuation_batch, self.menu, t_grid)  # (B, K)
        
        # ãƒãƒ¼ãƒ‰å‰²å½“
        selected_idx = torch.argmax(U, dim=1)  # (B,)
        selected_utility = torch.gather(U, 1, selected_idx.unsqueeze(1)).squeeze(1)  # (B,)
        
        # ä¾¡æ ¼ã‚’å–å¾—
        beta = torch.stack([elem.beta for elem in self.menu]).squeeze()  # (K,)
        selected_prices = beta[selected_idx]  # (B,)
        
        # IRåˆ¶ç´„ãƒã‚§ãƒƒã‚¯
        ir_mask = (selected_utility >= 0.0).float()  # (B,)
        
        # åå…¥è¨ˆç®—
        revenue = (selected_prices * ir_mask).mean()
        
        # åšç”Ÿè¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        welfare = selected_utility.mean()
        
        return {
            'assignments': selected_idx,
            'utilities': selected_utility,
            'prices': selected_prices,
            'revenue': revenue,
            'welfare': welfare,
            'ir_satisfied': ir_mask.mean()
        }


# ---- Stage 2: åŠ¹ç”¨ãƒ»æå¤± ------------------------------------------------

# ãƒ¡ãƒ‹ãƒ¥ãƒ¼lã«é–¢ã—ã¦ã€åŠ¹ç”¨ã‚’è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚
# u^(k)(v) = Î£_d v(s(Î¼_d^(k))) Â· w_d^(k) Â· exp{-Tr[Q(Î¼_d^(k))]âˆ«Î·} âˆ’ Î²^(k)  ï¼ˆEq.(21)ï¼‰
def utility_element(flow, v, elem: MenuElement, t_grid: torch.Tensor) -> torch.Tensor:
    """
    å˜ä¸€ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã®åŠ¹ç”¨ã‚’è¨ˆç®—
    
    Args:
        flow: BundleFlow
        v: è©•ä¾¡é–¢æ•°
        elem: MenuElement
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        
    Returns:
        utility: åŠ¹ç”¨
    """
    sT = flow.flow_forward(elem.mus, t_grid)     # (D,m)  ï¼ˆEq.(20)ï¼‰
    s = flow.round_to_bundle(sT)                 # (D,m)  ï¼ˆEq.(21)ã® s(Î¼)ï¼‰
    vals = torch.tensor([float(v.value(s[d])) for d in range(s.shape[0])],
                        device=s.device, dtype=torch.float64)  # å®‰å®šåŒ–: float64
    
    # æ•°å€¤å®‰å®šåŒ–ã•ã‚ŒãŸlog-sum-expå®Ÿè£…
    trQ = flow.Q(elem.mus).diagonal(dim1=-2, dim2=-1).sum(-1).to(torch.float64)   # (D,)
    integ = flow.eta_integral(t_grid).to(torch.float64)                           # ()
    log_w = torch.log_softmax(elem.logits.to(torch.float64), dim=0) - trQ * integ # (D,)
    
    # log-sum-exp: M = max(log_w), u = exp(M) * Î£_d exp(log_w - M) * v_d
    M = log_w.max()
    u = torch.exp(M) * torch.sum(torch.exp(log_w - M) * vals)                     # Î£_d e^{log_w-M} v_d
    
    # Î²â‰¥0: softplusã§æ­£ã®å€¤ã«åˆ¶é™ï¼ˆIRã¨æ•´åˆï¼‰
    beta = torch.nn.functional.softplus(elem.beta_raw)                            # Î²â‰¥0
    
    return (u.to(s.dtype) - beta)


# ãƒãƒƒãƒå‡¦ç†ç‰ˆï¼šå…¨menu elementsã‚’ä¸€åº¦ã«å‡¦ç†
# U[b,k] = u^(k)(v_b)  ï¼ˆEq.(21) ã‚’å…¨çµ„ã§ï¼‰
def utilities_matrix_batched(flow, V: List, menu: List[MenuElement], t_grid: torch.Tensor, verbose: bool = False, debug: bool = False, v0_test=None) -> torch.Tensor:
    """
    é«˜é€Ÿãƒãƒƒãƒå‡¦ç†ç‰ˆï¼šå…¨menu elementsã®musã‚’çµ±åˆã—ã¦ä¸€åº¦ã«å‡¦ç†
    é€Ÿåº¦: O(B * K * D) â†’ O(B * 1)ã®flow_forwardå‘¼ã³å‡ºã—
    
    Args:
        flow: BundleFlow
        V: è©•ä¾¡é–¢æ•°ã®ãƒªã‚¹ãƒˆ
        menu: MenuElementã®ãƒªã‚¹ãƒˆ
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        verbose: è©³ç´°ãƒ­ã‚°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        v0_test: ãƒ†ã‚¹ãƒˆç”¨è©•ä¾¡é–¢æ•°
        
    Returns:
        U: åŠ¹ç”¨è¡Œåˆ— (B, K)
    """
    K = len(menu)
    B = len(V)
    device = t_grid.device
    
    if verbose:
        print(f"  [Batched] Collecting all mus from {K} menu elements...", flush=True)
    
    # nullè¦ç´ ï¼ˆæœ€å¾Œã®è¦ç´ ã€D=1ï¼‰ã‚’åˆ†é›¢
    menu_main = menu[:-1]  # é€šå¸¸ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ï¼ˆD=8ãªã©ï¼‰
    null_elem = menu[-1]   # nullè¦ç´ ï¼ˆD=1ï¼‰
    K_main = len(menu_main)
    
    if verbose:
        print(f"  [Batched] K={K}, K_main={K_main}, menu length={len(menu)}", flush=True)
        print(f"  [Batched] Splitting: menu_main has {K_main} elements, null is 1 element", flush=True)
    
    # é€šå¸¸ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã®musã‚’çµ±åˆ: [(D, m)] * K_main -> (K_main, D, m)
    all_mus = torch.stack([elem.mus for elem in menu_main])  # (K_main, D, m)
    all_weights = torch.stack([elem.weights for elem in menu_main])  # (K_main, D)
    all_betas = torch.stack([elem.beta for elem in menu_main]).squeeze()  # (K_main,)
    
    K_main, D, m = all_mus.shape
    
    if verbose:
        print(f"  [Batched] Running flow_forward on {K_main*D} bundles at once...", flush=True)
        # Î¼ã®ç¯„å›²ã‚’ç¢ºèª
        print(f"  [Batched] all_mus range: [{all_mus.min().item():.4f}, {all_mus.max().item():.4f}], mean={all_mus.mean().item():.4f}", flush=True)
    
    # å…¨ã¦ã®Î¼ã‚’ãƒãƒƒãƒå‡¦ç†: (K_main, D, m) -> (K_main*D, m)
    mus_flat = all_mus.view(K_main * D, m)
    sT_flat = flow.flow_forward(mus_flat, t_grid)  # (K_main*D, m) - ä¸€åº¦ã®å‘¼ã³å‡ºã—ï¼
    
    if verbose:
        print(f"  [Batched] sT_flat range BEFORE rounding: [{sT_flat.min().item():.4f}, {sT_flat.max().item():.4f}], mean={sT_flat.mean().item():.4f}", flush=True)
    
    s_flat = flow.round_to_bundle(sT_flat)  # (K_main*D, m)
    
    if verbose:
        print(f"  [Batched] s_flat range AFTER rounding: [{s_flat.min().item():.4f}, {s_flat.max().item():.4f}], mean={s_flat.mean().item():.4f}", flush=True)
    
    # log_density_weightã‚‚ãƒãƒƒãƒå‡¦ç†
    log_density_flat = flow.log_density_weight(mus_flat, t_grid)  # (K_main*D,)
    # å®‰å®šåŒ–ï¼šlog_densityã‚’ã‚¯ãƒªãƒƒãƒ—ï¼ˆexpçˆ†ç™ºã‚’é˜²æ­¢ï¼‰
    log_density_flat = torch.clamp(log_density_flat, -10.0, 0.0)  # å¯†åº¦é‡ã¿ â‰¤ 1.0
    log_density = log_density_flat.view(K_main, D)  # (K_main, D)
    
    # nullè¦ç´ ã‚‚ä¸€åº¦ã«å‡¦ç†
    null_sT = flow.flow_forward(null_elem.mus, t_grid)  # (1, m)
    null_s = flow.round_to_bundle(null_sT)  # (1, m)
    null_log_density = flow.log_density_weight(null_elem.mus, t_grid)  # (1,)
    null_log_density = torch.clamp(null_log_density, -10.0, 0.0)  # å®‰å®šåŒ–
    null_weight = null_elem.weights  # (1,)
    null_beta = null_elem.beta  # scalar
    
    # å„valuationã«ã¤ã„ã¦åŠ¹ç”¨ã‚’è¨ˆç®—
    # æ³¨æ„ï¼šmenuã¯ K_main + 1 å€‹ï¼ˆnullå«ã‚€ï¼‰
    U = torch.zeros(B, K, device=device, dtype=torch.float32)  # K = K_main + 1
    
    # s_flatã‚’CPUã«ä¸€åº¦ã ã‘è»¢é€ï¼ˆå…¨valuationã§å…±æœ‰ï¼‰
    s_flat_cpu = s_flat.cpu()
    
    for i, v in enumerate(V):
        if verbose and i % 10 == 0:
            print(f"  [Batched] Processing valuation {i+1}/{B}...", flush=True)
        
        # å…¨bundlesã®ä¾¡å€¤ã‚’ä¸€åº¦ã«è¨ˆç®—
        if hasattr(v, 'batch_value'):
            vals_flat = v.batch_value(s_flat_cpu)  # (K_main*D,)
            
            # ãƒ‡ãƒãƒƒã‚°ï¼šè¤‡æ•°ã®valuationã‚’ãƒã‚§ãƒƒã‚¯
            if debug and i < 3:  # æœ€åˆã®3å€‹ã®valuationã‚’ãƒã‚§ãƒƒã‚¯
                non_zero_count = (vals_flat > 0).sum().item()
                print(f"  [DEBUG] Valuation {i}: batch_value min={vals_flat.min().item():.4f}, max={vals_flat.max().item():.4f}, non-zero={non_zero_count}/{len(vals_flat)}", flush=True)
                print(f"  [DEBUG] Valuation {i}: num_atoms={len(v.atoms)}, max_price={max(p for _, p in v.atoms):.4f}", flush=True)
            
            vals_flat = vals_flat.to(device)  # GPUã«æˆ»ã™
        else:
            # fallback: é€æ¬¡å‡¦ç†
            vals_flat = torch.tensor([v.value(s_flat_cpu[j]) for j in range(K_main*D)],
                                    device=device, dtype=torch.float32)
        
        vals = vals_flat.view(K_main, D)  # (K_main, D)
        
        # å„menu elementã®åŠ¹ç”¨ã‚’è¨ˆç®—ï¼ˆlog-sum-expï¼‰
        log_w = torch.log(all_weights + 1e-10)  # (K_main, D)
        log_weights = log_w + log_density  # (K_main, D)
        
        # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®valuationã§è©³ç´°ã‚’å‡ºåŠ›ï¼ˆdebugãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ã‚‹ã¨ãï¼‰
        if __debug__ and debug and i == 0:
            # ç°¡æ½”ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®ã¿
            print(f"  [DEBUG] s_flat shape: {s_flat.shape}, unique bundles: {len(torch.unique(s_flat, dim=0))}", flush=True)
            print(f"  [DEBUG] Number of valuation atoms: {len(v.atoms)}", flush=True)
            print(f"  [DEBUG] vals range: [{vals.min().item():.4f}, {vals.max().item():.4f}]", flush=True)
        
        # log-sum-exp per menu element
        M = torch.max(log_weights, dim=1, keepdim=True)[0]  # (K_main, 1)
        weighted_sum = (torch.exp(log_weights - M) * vals).sum(dim=1)  # (K_main,)
        u_main = torch.exp(M.squeeze(1)) * weighted_sum - all_betas  # (K_main,)
        
        if __debug__ and debug and i == 0:
            print(f"  [DEBUG] u_main range: [{u_main.min().item():.4f}, {u_main.max().item():.4f}]", flush=True)
        
        # nullè¦ç´ ã®åŠ¹ç”¨ã‚’è¨ˆç®—ï¼ˆäº‹å‰è¨ˆç®—æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        if hasattr(v, 'value'):
            null_val = v.value(null_s[0])
        else:
            null_val = 0.0
        log_w_null = torch.log(null_weight + 1e-10)
        log_weight_null = log_w_null + null_log_density
        u_null = torch.exp(log_weight_null[0]) * null_val - null_beta
        
        # çµåˆ: [u_main, u_null]
        if debug and i == 0:
            print(f"  [DEBUG] u_main shape: {u_main.shape}, u_null shape: {u_null.shape if isinstance(u_null, torch.Tensor) else 'scalar'}", flush=True)
            print(f"  [DEBUG] U shape: {U.shape}, U[i, :-1] will be shape {U[i, :-1].shape}", flush=True)
            print(f"  [DEBUG] Assigning u_main (len={len(u_main)}) to U[{i}, :-1] (len={len(U[i, :-1])})", flush=True)
        
        U[i, :-1] = u_main
        U[i, -1] = u_null
        
        if debug and i == 0:
            print(f"  [DEBUG] After assignment: U[{i}] range = [{U[i].min().item():.4f}, {U[i].max().item():.4f}]", flush=True)
            print(f"  [DEBUG] U[{i}, -1] (null) = {U[i, -1].item():.4f}", flush=True)
    
    if verbose:
        print(f"  [Batched] Utilities computed: {U.shape}", flush=True)
    
    return U  # (B, K)


# ãƒãƒªãƒ¥ãƒ¼é›†åˆVã¨ã€å…¨ã¦ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼lã«é–¢ã—ã¦ã€åŠ¹ç”¨ã‚’è¡Œåˆ—è¡¨ç¤ºã—ã¦ã„ã‚‹ã€‚
# U[b,k] = u^(k)(v_b)  ï¼ˆEq.(21) ã‚’å…¨çµ„ã§ï¼‰
def utilities_matrix(flow, V: List, menu: List[MenuElement], t_grid: torch.Tensor, verbose: bool = False, debug: bool = False, v0_test=None) -> torch.Tensor:
    """
    åŠ¹ç”¨è¡Œåˆ—ã‚’è¨ˆç®—
    
    Args:
        flow: BundleFlow
        V: è©•ä¾¡é–¢æ•°ã®ãƒªã‚¹ãƒˆ
        menu: MenuElementã®ãƒªã‚¹ãƒˆ
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        verbose: è©³ç´°ãƒ­ã‚°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        v0_test: ãƒ†ã‚¹ãƒˆç”¨è©•ä¾¡é–¢æ•°
        
    Returns:
        U: åŠ¹ç”¨è¡Œåˆ— (B, K)
    """
    # ãƒãƒƒãƒå‡¦ç†ç‰ˆã‚’ä½¿ç”¨
    return utilities_matrix_batched(flow, V, menu, t_grid, verbose=verbose, debug=debug, v0_test=v0_test)


# å­¦ç¿’æ™‚ã®é€£ç¶šå‰²ã‚Šå½“ã¦ã‚’è¿”ã™ã€‚
# z^(k)(v) = SoftMax_k( Î» Â· u^(k)(v) )  ï¼ˆEq.(23)ï¼‰
def soft_assignment(U: torch.Tensor, lam: float) -> torch.Tensor:
    """
    ã‚½ãƒ•ãƒˆå‰²å½“ã‚’è¨ˆç®—
    
    Args:
        U: åŠ¹ç”¨è¡Œåˆ— (B, K)
        lam: æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        Z: ã‚½ãƒ•ãƒˆå‰²å½“ (B, K)
    """
    return torch.softmax(lam * U, dim=1)


# æœŸå¾…æç›Šã®è² ã®å€¤ã‚’æœ€å°åŒ–ã—ã¦ã„ã‚‹ã€‚
def revenue_loss(flow, V: List, menu: List[MenuElement], t_grid: torch.Tensor, lam: float = 0.1, 
                 verbose: bool = False, debug: bool = False, v0_test=None,
                 use_gumbel: bool = False, tau: float = 0.1) -> torch.Tensor:
    """
    Stage 2ã®åç›Šæå¤±é–¢æ•°
    
    Args:
        flow: FlowModel
        V: Valuations
        menu: List[MenuElement]
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        lam: Softmaxæ¸©åº¦ï¼ˆuse_gumbel=Falseã®æ™‚ã®ã¿ä½¿ç”¨ï¼‰
        verbose: è©³ç´°ãƒ­ã‚°
        debug: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
        v0_test: ãƒ†ã‚¹ãƒˆç”¨valuation
        use_gumbel: Trueãªã‚‰Gumbel-Softmax + STEã€Falseãªã‚‰Softmax relaxation
        tau: Gumbel-Softmaxæ¸©åº¦ï¼ˆuse_gumbel=Trueã®æ™‚ã®ã¿ä½¿ç”¨ï¼‰
        
    Returns:
        -revenue (æå¤±)
    """
    # LRev = -(1/|V|) Î£_v Î£_k z_k(v) Î²_k  ï¼ˆEq.(22)ï¼‰
    if verbose:
        print(f"  Computing utilities matrix ({len(V)} valuations Ã— {len(menu)} menu elements)...", flush=True)
    U = utilities_matrix(flow, V, menu, t_grid, verbose=verbose, debug=debug, v0_test=v0_test)  # (B,K)
    beta = torch.stack([elem.beta for elem in menu]).squeeze()     # (K,)
    
    if use_gumbel:
        # Gumbel-Softmax + STE: Forwardæ™‚ã«hard argmaxã€Backwardæ™‚ã«softå‹¾é…
        if verbose:
            print(f"  Computing Gumbel-Softmax assignment (tau={tau}, hard=True)...", flush=True)
        
        from bf.utils import gumbel_softmax
        if debug:
            print(f"  [DEBUG-GUMBEL] U shape: {U.shape}, U range: [{U.min().item():.4f}, {U.max().item():.4f}]", flush=True)
        Z = gumbel_softmax(U, tau=tau, hard=True, dim=1)  # (B, K) one-hot
        if debug:
            print(f"  [DEBUG-GUMBEL] Z shape: {Z.shape}, Z sum per row: {Z.sum(dim=1)[:5].tolist()}", flush=True)
        
        # IRåˆ¶ç´„ã®æ˜ç¤ºçš„é©ç”¨: é¸æŠã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®åŠ¹ç”¨ãŒè² ãªã‚‰åç›Šã‚¼ãƒ­
        selected_idx = torch.argmax(Z, dim=1)  # (B,)
        selected_utility = torch.gather(U, 1, selected_idx.unsqueeze(1)).squeeze(1)  # (B,)
        ir_mask = (selected_utility >= 0.0).float()  # (B,)
        
        # åç›Šè¨ˆç®—: IRåˆ¶ç´„ã‚’æº€ãŸã•ãªã„å ´åˆã¯0
        if debug:
            print(f"  [DEBUG-GUMBEL] beta shape: {beta.shape}, beta range: [{beta.min().item():.4f}, {beta.max().item():.4f}]", flush=True)
        rev_per_buyer = (Z * beta.unsqueeze(0)).sum(dim=1) * ir_mask  # (B,)
        rev = rev_per_buyer.mean()
        
        if debug:
            print(f"  [DEBUG-REV-GUMBEL] U range: [{U.min().item():.4f}, {U.max().item():.4f}]", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] U mean: {U.mean().item():.4f}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] beta range: [{beta.min().item():.4f}, {beta.max().item():.4f}]", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] beta mean: {beta.mean().item():.4f}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Z shape: {Z.shape}, is one-hot: {(Z.sum(dim=1) == 1.0).all().item()}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Selected indices: {selected_idx[:5].tolist()}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Selected utilities: {selected_utility[:5].tolist()}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] IR constraint satisfied: {ir_mask.sum().item()}/{len(ir_mask)}", flush=True)
            print(f"  [DEBUG-REV-GUMBEL] Revenue per buyer (first 5): {rev_per_buyer[:5].tolist()}", flush=True)
    else:
        # å¾“æ¥ã®Softmax relaxation
        if verbose:
            print(f"  Computing soft assignment (lambda={lam})...", flush=True)
        Z = soft_assignment(U, lam)  # (B,K)
        rev = (Z * beta.unsqueeze(0)).sum(dim=1).mean()
        
        if debug:
            print(f"  [DEBUG-REV] Z shape: {Z.shape}, beta shape: {beta.shape}", flush=True)
            print(f"  [DEBUG-REV] Z[0] (first valuation selection probs): min={Z[0].min().item():.6f}, max={Z[0].max().item():.6f}", flush=True)
            print(f"  [DEBUG-REV] Z[:, -1] (null selection): min={Z[:, -1].min().item():.6f}, max={Z[:, -1].max().item():.6f}, mean={Z[:, -1].mean().item():.6f}", flush=True)
            print(f"  [DEBUG-REV] beta range: [{beta.min().item():.4f}, {beta.max().item():.4f}]", flush=True)
            print(f"  [DEBUG-REV] beta[-1] (null price): {beta[-1].item():.6f}", flush=True)
            print(f"  [DEBUG-REV] (Z * beta) per valuation: {(Z * beta.unsqueeze(0)).sum(dim=1)[:5].tolist()}", flush=True)
    
    if verbose:
        print(f"  Revenue: {rev.item():.6f} ({'Gumbel+STE' if use_gumbel else 'Softmax'})", flush=True)
    return -rev


# ãƒ†ã‚¹ãƒˆæ™‚
@torch.no_grad()
def visualize_menu(flow, menu: List[MenuElement], t_grid: torch.Tensor, 
                   max_items: int = 10, device: torch.device = None) -> None:
    """
    ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®å†…å®¹ã‚’å¯è¦–åŒ–ï¼ˆã©ã®å•†å“ã®çµ„ã¿åˆã‚ã›ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
    
    Args:
        flow: BundleFlow
        menu: MenuElementã®ãƒªã‚¹ãƒˆ
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        max_items: è¡¨ç¤ºã™ã‚‹æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°
        device: ãƒ‡ãƒã‚¤ã‚¹
    """
    if device is None:
        device = next(menu[0].parameters()).device
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ MENU VISUALIZATION (showing first {max_items} items)")
    print(f"{'='*60}")
    
    for k, elem in enumerate(menu[:max_items]):
        # ãƒãƒ³ãƒ‰ãƒ«ç”Ÿæˆï¼ˆÎ¼ã‹ã‚‰ï¼‰
        with torch.no_grad():
            s_T = flow.flow_forward(elem.mus, t_grid)  # (D, m)
            s = flow.round_to_bundle(s_T)  # (D, m) -> discrete bundles
            
        # å„ãƒãƒ³ãƒ‰ãƒ«ã®å†…å®¹ã‚’è¡¨ç¤º
        beta_val = elem.beta.item()
        print(f"\nğŸ½ï¸  Menu Item {k+1}: Price = {beta_val:.4f}")
        
        # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒãƒ³ãƒ‰ãƒ«ã‚’æŠ½å‡º
        unique_bundles = []
        for d in range(s.shape[0]):
            bundle = s[d].cpu().numpy()
            bundle_tuple = tuple(bundle.astype(int))
            if bundle_tuple not in unique_bundles:
                unique_bundles.append(bundle_tuple)
        
        print(f"   ğŸ“¦ Generated bundles ({len(unique_bundles)} unique):")
        for i, bundle in enumerate(unique_bundles[:5]):  # æœ€å¤§5å€‹è¡¨ç¤º
            items = [j for j, val in enumerate(bundle) if val == 1]
            if items:
                items_str = ", ".join([f"Item_{j}" for j in items])
                print(f"      {i+1}. [{items_str}]")
            else:
                print(f"      {i+1}. [Empty bundle]")
        
        if len(unique_bundles) > 5:
            print(f"      ... and {len(unique_bundles)-5} more bundles")
    
    if len(menu) > max_items:
        print(f"\n... and {len(menu)-max_items} more menu items")
    
    print(f"{'='*60}\n")


# ãƒ†ã‚¹ãƒˆæ™‚ã¯SoftMaxã§ã¯ãªãã€argmaxã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚
def infer_choice(flow, v, menu: List[MenuElement], t_grid: torch.Tensor) -> int:
    """
    æ¨è«–æ™‚ã®é¸æŠã‚’è¨ˆç®—
    
    Args:
        flow: BundleFlow
        v: è©•ä¾¡é–¢æ•°
        menu: MenuElementã®ãƒªã‚¹ãƒˆ
        t_grid: æ™‚é–“ã‚°ãƒªãƒƒãƒ‰
        
    Returns:
        choice: é¸æŠã•ã‚ŒãŸãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    """
    # æ¨è«–æ™‚ã¯ argmaxï¼ˆæœ¬æ–‡ Sec.3.3 / Eq.(23) ã®ãƒãƒ¼ãƒ‰ç‰ˆï¼‰
    U = torch.stack([utility_element(flow, v, elem, t_grid) for elem in menu])  # (K,)
    return int(torch.argmax(U).item())
