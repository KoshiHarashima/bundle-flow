# Rectified Flowå…¥é–€ï¼šçµŒæ¸ˆå­¦è€…ã®ãŸã‚ã®æŠ€è¡“è§£èª¬

**å¯¾è±¡èª­è€…**: çµ±è¨ˆå­¦ã«ã¯ç²¾é€šã—ã¦ã„ã‚‹ãŒã€æ©Ÿæ¢°å­¦ç¿’ã«ã¯ä¸æ…£ã‚ŒãªçµŒæ¸ˆå­¦ç ”ç©¶è€…

---

## Executive Summary

Rectified Flowã¯ã€**é€£ç¶šåˆ†å¸ƒã‹ã‚‰é›¢æ•£çš„ãªçµ„ã¿åˆã‚ã›æ§‹é€ ã¸ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**ã‚’å¯èƒ½ã«ã™ã‚‹æ·±å±¤å­¦ç¿’æ‰‹æ³•ã§ã‚ã‚‹ã€‚BundleFlowã§ã¯ã€ã“ã®æŠ€è¡“ã‚’ç”¨ã„ã¦ã€Œç„¡é™ã«è¿‘ã„æ•°ã®ãƒãƒ³ãƒ‰ãƒ«å€™è£œã€ã‹ã‚‰ã€Œè²·ã„æ‰‹ã«ã¨ã£ã¦é­…åŠ›çš„ãªãƒãƒ³ãƒ‰ãƒ«ã€ã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆã™ã‚‹ã€‚æœ¬ç¨¿ã§ã¯ã€Rectified Flowã®æ•°å­¦çš„åŸºç¤ã‚’ã€çµŒæ¸ˆå­¦è€…ã«é¦´æŸ“ã¿ã®ã‚ã‚‹ç¢ºç‡è«–ãƒ»æœ€é©åŒ–ç†è«–ã®è¦³ç‚¹ã‹ã‚‰è§£èª¬ã™ã‚‹ã€‚

---

## 1. å•é¡Œè¨­å®šï¼šçµ„ã¿åˆã‚ã›çˆ†ç™ºã¨ã®æˆ¦ã„

### 1.1 ãƒãƒ³ãƒ‰ãƒ«ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã®èª²é¡Œ

`m` å€‹ã®å•†å“ãŒã‚ã‚‹ã¨ãã€å¯èƒ½ãªãƒãƒ³ãƒ‰ãƒ«ã®æ•°ã¯ï¼š

$$
|\mathcal{S}| = 2^m
$$

**å…·ä½“ä¾‹**:
- `m = 10`: 1,024é€šã‚Š
- `m = 50`: 1,125,899,906,842,624é€šã‚Šï¼ˆ1000å…†è¶…ï¼‰
- `m = 100`: 10Â³â° é€šã‚Šï¼ˆå®‡å®™ã®åŸå­ã®æ•°ã‚ˆã‚Šå¤šã„ï¼‰

**å•é¡Œ**: ã‚»ãƒ©ãƒ¼ã¯ã©ã®ãƒãƒ³ãƒ‰ãƒ«ã‚’æä¾›ã™ã¹ãã‹ï¼Ÿ

### 1.2 å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

#### (A) å…¨æ¢ç´¢

ã™ã¹ã¦ã®ãƒãƒ³ãƒ‰ãƒ«ã«ã¤ã„ã¦åç›Šã‚’è¨ˆç®—ï¼š

$$
\max_{S_1, \ldots, S_K \in \{0,1\}^m} \mathbb{E}_{v \sim \mathcal{V}}[\text{Revenue}(S_1, \ldots, S_K; v)]
$$

**å•é¡Œ**: `m` ãŒå¤§ãã„ã¨è¨ˆç®—ä¸å¯èƒ½ï¼ˆNPå›°é›£ï¼‰

#### (B) è²ªæ¬²æ³•ãƒ»ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯

- VCG ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®è¿‘ä¼¼
- ç·šå½¢ç·©å’Œ + æ•´æ•°è¨ˆç”»æ³•
- ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒ†ãƒƒãƒ‰ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°

**å•é¡Œ**: 
- æœ€é©æ€§ã®ä¿è¨¼ãŒãªã„
- ãƒã‚¤ãƒ¤ãƒ¼ã®è©•ä¾¡å€¤åˆ†å¸ƒã«ä¾å­˜

### 1.3 BundleFlowã®ææ¡ˆ

**ã‚¢ã‚¤ãƒ‡ã‚¢**: ã€Œè‰¯ã„ãƒãƒ³ãƒ‰ãƒ«ã€ã‚’ç”Ÿæˆã™ã‚‹ç¢ºç‡åˆ†å¸ƒã‚’**å­¦ç¿’**ã™ã‚‹

$$
\mu \xrightarrow{\text{å­¦ç¿’æ¸ˆã¿å¤‰æ›}} S \in \{0,1\}^m
$$

where `Î¼` ã¯é€£ç¶šåˆ†å¸ƒï¼ˆæ‰±ã„ã‚„ã™ã„ï¼‰ã€`S` ã¯é›¢æ•£ãƒãƒ³ãƒ‰ãƒ«ï¼ˆå®Ÿéš›ã«å¿…è¦ï¼‰

**åˆ©ç‚¹**:
- ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§å¤šæ§˜ãªãƒãƒ³ãƒ‰ãƒ«ã‚’ç”Ÿæˆ
- è©•ä¾¡å€¤åˆ†å¸ƒã«é©å¿œ
- è¨ˆç®—å¯èƒ½

---

## 2. Rectified Flow: é€£ç¶šâ†’é›¢æ•£ã®æ©‹æ¸¡ã—

### 2.1 åŸºæœ¬æ¦‚å¿µ

**ç›®æ¨™**: å˜ç´”ãªé€£ç¶šåˆ†å¸ƒ `Î¼` ã‚’ã€é›¢æ•£ãƒãƒ³ãƒ‰ãƒ«åˆ†å¸ƒã«å¤‰æ›ã™ã‚‹å†™åƒã‚’å­¦ç¿’

#### ã‚¹ãƒ†ãƒƒãƒ—1: åˆæœŸåˆ†å¸ƒ `Î¼â‚€`

`D` å€‹ã®æ”¯æŒç‚¹ã‚’æŒã¤Gaussianæ··åˆåˆ†å¸ƒï¼š

$$
\mu_0 = \sum_{d=1}^D w_d \cdot \mathcal{N}(\mu_d, \sigma_d^2 I)
$$

where:
- `w_d`: æ··åˆé‡ã¿ï¼ˆ`Î£ w_d = 1`ï¼‰
- `Î¼_d âˆˆ â„^m`: å„æˆåˆ†ã®ä¸­å¿ƒ
- `Ïƒ_d > 0`: åˆ†æ•£

**ç›´æ„Ÿ**: `Î¼â‚€` ã¯ `m` æ¬¡å…ƒç©ºé–“å†…ã®ã€Œç¢ºç‡çš„ãªç‚¹ã€ã‚’è¡¨ã™

#### ã‚¹ãƒ†ãƒƒãƒ—2: çµ‚ç‚¹åˆ†å¸ƒ `s`

é›¢æ•£çš„ãª0/1ãƒãƒ³ãƒ‰ãƒ«ï¼š

$$
s \in \{0,1\}^m
$$

#### ã‚¹ãƒ†ãƒƒãƒ—3: Rectified Flow

æ™‚é–“ `t âˆˆ [0,1]` ã§é€£ç¶šçš„ã«å¤‰æ›ï¼š

$$
\frac{ds_t}{dt} = \phi(t, s_t, s_0)
$$

åˆæœŸæ¡ä»¶: `s_0 ~ Î¼â‚€`, çµ‚ç‚¹: `s_1 â‰ˆ s` ï¼ˆé›¢æ•£ãƒãƒ³ãƒ‰ãƒ«ï¼‰

### 2.2 çµŒæ¸ˆå­¦çš„è§£é‡ˆ

#### é¡ä¼¼æ¦‚å¿µ: å‡è¡¡ã¸ã®èª¿æ•´éç¨‹

çµŒæ¸ˆå­¦ã«ãŠã‘ã‚‹**tatonnement process**ï¼ˆæ¨¡ç´¢éç¨‹ï¼‰ã¨é¡ä¼¼ï¼š

```
åˆæœŸä¾¡æ ¼ pâ‚€ â†’ å¸‚å ´ãƒ¡ã‚«ãƒ‹ã‚ºãƒ  â†’ å‡è¡¡ä¾¡æ ¼ p*
```

Rectified Flowã§ã¯ï¼š

```
é€£ç¶šåˆ†å¸ƒ Î¼ â†’ Flowå¤‰æ› Ï† â†’ é›¢æ•£ãƒãƒ³ãƒ‰ãƒ« s
```

ä¸¡æ–¹ã¨ã‚‚ã€ŒåˆæœŸçŠ¶æ…‹ã‹ã‚‰æœ›ã¾ã—ã„çµ‚çŠ¶æ…‹ã¸ã®é€£ç¶šçš„ãªç§»è¡Œã€ã‚’è¨˜è¿°ã€‚

#### ç¢ºç‡çš„ãªçµŒè·¯

æ™‚åˆ» `t` ã§ã®çŠ¶æ…‹ã¯**ç¢ºç‡çš„**ï¼š

$$
s_t \sim \text{Law}(s_t | s_0)
$$

ã“ã‚Œã¯ç¢ºç‡å¾®åˆ†æ–¹ç¨‹å¼ï¼ˆSDEï¼‰ã§ã¯ãªãã€å¸¸å¾®åˆ†æ–¹ç¨‹å¼ï¼ˆODEï¼‰ã®è§£ã®åˆ†å¸ƒã€‚

---

## 3. æ•°å­¦çš„å®šå¼åŒ–

### 3.1 ãƒ™ã‚¯ãƒˆãƒ«å ´ `Ï†`

**å®šç¾©**: `Ï†: [0,1] Ã— â„^m Ã— â„^m â†’ â„^m` ã¯ä»¥ä¸‹ã®å½¢ï¼š

$$
\phi(t, s_t, s_0) = \eta(t) \cdot Q(s_0) \cdot s_t
$$

where:
- `Î·(t)`: ã‚¹ã‚«ãƒ©ãƒ¼é–¢æ•°ï¼ˆæ™‚é–“ä¾å­˜ã®ã€Œé€Ÿåº¦ã€ï¼‰
- `Q(s_0) âˆˆ â„^{mÃ—m}`: è¡Œåˆ—é–¢æ•°ï¼ˆåˆæœŸçŠ¶æ…‹ä¾å­˜ã®ã€Œæ–¹å‘ã€ï¼‰

**ç›´æ„Ÿ**:
- `Î·(t)`: æ™‚åˆ» `t` ã§ã©ã‚Œãã‚‰ã„é€Ÿãå¤‰åŒ–ã™ã‚‹ã‹
- `Q(s_0)`: ã©ã®æ–¹å‘ã«å¤‰åŒ–ã™ã‚‹ã‹ï¼ˆåˆæœŸå€¤ã«ä¾å­˜ï¼‰

### 3.2 ODEã®ç©åˆ†

åˆæœŸå€¤å•é¡Œï¼š

$$
\begin{cases}
\frac{ds_t}{dt} = \phi(t, s_t, s_0) \\
s_t|_{t=0} = s_0
\end{cases}
$$

**æ•°å€¤è§£æ³•**ï¼ˆEuleræ³•ï¼‰:

$$
s_{t+\Delta t} = s_t + \Delta t \cdot \phi(t, s_t, s_0)
$$

æ™‚é–“ã‚°ãƒªãƒƒãƒ‰ `0 = tâ‚€ < tâ‚ < ... < t_N = 1` ã§åå¾©ï¼š

$$
s_{t_{i+1}} = s_{t_i} + (t_{i+1} - t_i) \cdot \phi(t_i, s_{t_i}, s_0)
$$

**çµ±è¨ˆçš„è§£é‡ˆ**: ã“ã‚Œã¯ç¢ºç‡å¤‰æ•°ã®æ±ºå®šè«–çš„å¤‰æ›ï¼ˆ`s_0` ãŒãƒ©ãƒ³ãƒ€ãƒ ãªã®ã§ `s_1` ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ï¼‰

### 3.3 é›¢æ•£åŒ–ï¼ˆRoundingï¼‰

çµ‚ç‚¹ `s_T âˆˆ â„^m` ã¯é€£ç¶šå€¤ãªã®ã§ã€é›¢æ•£ãƒãƒ³ãƒ‰ãƒ«ã«å¤‰æ›ï¼š

$$
s_i = \mathbb{1}\{s_{T,i} \geq 0.5\}, \quad i = 1, \ldots, m
$$

**çµŒæ¸ˆå­¦çš„é¡æ¨**: é–¾å€¤ãƒ«ãƒ¼ãƒ«ï¼ˆä¾‹ï¼šæŠ•ç¥¨ã§ã®éåŠæ•°ã€ä¿¡ç”¨ã‚¹ã‚³ã‚¢ã§ã®æ‰¿èªåŸºæº–ï¼‰

---

## 4. å­¦ç¿’ç›®çš„é–¢æ•°

### 4.1 Rectification Loss

**ç›®æ¨™**: Flow `Ï†` ã‚’å­¦ç¿’ã—ã¦ã€æœ€çŸ­çµŒè·¯ã§ `s_0 â†’ s_T` ã‚’å®Ÿç¾

$$
\mathcal{L}_{\text{flow}} = \mathbb{E}_{s_0, s_T, t} \left[ \| (s_T - s_0) - \phi(t, s_t, s_0) \|^2 \right]
$$

where:
- `s_0 ~ Î¼â‚€` ï¼ˆåˆæœŸåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- `s = round(s_0)` ï¼ˆé›¢æ•£åŒ–ï¼‰
- `s_T ~ N(s, Ïƒ_zÂ² I)` ï¼ˆãƒã‚¤ã‚ºä»˜åŠ ã€Eq. 14ï¼‰
- `t ~ Uniform[0,1]` ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãªæ™‚åˆ»ï¼‰
- `s_t = t Â· s_T + (1-t) Â· s_0` ï¼ˆç·šå½¢è£œé–“ã€Eq. 16ï¼‰

### 4.2 ç›´æ„Ÿçš„ç†è§£

#### (1) ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: `s_T - s_0`

ã‚‚ã— `Ï†` ãŒå®Œç’§ãªã‚‰ã€

$$
s_T = s_0 + \int_0^1 \phi(t, s_t, s_0) dt
$$

å¾®åˆ†ã™ã‚‹ã¨ã€

$$
\phi(t, s_t, s_0) = \frac{d s_t}{dt} = s_T - s_0 \quad (\text{å®šæ•°})
$$

ã¤ã¾ã‚Šã€ç†æƒ³çš„ãª Flow ã¯**ç›´ç·šçš„ãªçµŒè·¯**ï¼ˆRectified = çŸ¯æ­£ã•ã‚ŒãŸï¼‰

#### (2) å­¦ç¿’ã®ä»•çµ„ã¿

ãƒ©ãƒ³ãƒ€ãƒ ãªæ™‚åˆ» `t` ã§ã€`Ï†` ã®äºˆæ¸¬ã¨å®Ÿéš›ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ¯”è¼ƒï¼š

```
äºˆæ¸¬:   Ï†(t, s_t, s_0)
æ­£è§£:   s_T - s_0
æå¤±:   || äºˆæ¸¬ - æ­£è§£ ||Â²
```

ã“ã‚Œã¯**regression problem**ï¼ˆå›å¸°å•é¡Œï¼‰ï¼

### 4.3 ãªãœãƒã‚¤ã‚º `Ïƒ_z` ãŒå¿…è¦ã‹

`s` ã¯é›¢æ•£çš„ï¼ˆ`{0,1}^m`ï¼‰ãªã®ã§ã€ãã®ã¾ã¾ã§ã¯å­¦ç¿’ãŒé›£ã—ã„ã€‚

**Solution**: é›¢æ•£ç‚¹ã®å‘¨è¾ºã« Gaussian ãƒã‚¤ã‚ºã‚’è¿½åŠ ï¼š

$$
s_T \sim \mathcal{N}(s, \sigma_z^2 I)
$$

**åŠ¹æœ**:
- é€£ç¶šçš„ãªåˆ†å¸ƒã‚’ä½œã‚‹ â†’ å‹¾é…ãŒæµã‚Œã‚„ã™ã„
- åŒã˜ãƒãƒ³ãƒ‰ãƒ« `s` ã§ã‚‚è¤‡æ•°ã® `s_T` ãŒç”Ÿæˆ â†’ ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
- `Ïƒ_z` ãŒå°ã•ã„ â†’ `s_T â‰ˆ s` ï¼ˆé›¢æ•£ã«è¿‘ã„ï¼‰

**çµŒæ¸ˆå­¦çš„é¡æ¨**: Logit shock in discrete choice models

### 4.4 å¯†åº¦é‡ã¿é …

Flowã®å¤‰æ›ã§ç¢ºç‡å¯†åº¦ãŒã©ã†å¤‰ã‚ã‚‹ã‹ã‚’è¿½è·¡ï¼š

$$
\log p(s_T) = \log p(s_0) - \int_0^T \text{Tr}[Q(s_0)] \cdot \eta(t) \, dt
$$

**Jacobian ã®è¡Œåˆ—å¼**:

$$
\frac{\partial \phi}{\partial s_t} = \eta(t) \cdot Q(s_0)
$$

ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆå¯¾è§’å’Œï¼‰:

$$
\text{div} \, \phi = \text{Tr}[Q(s_0)] \cdot \eta(t)
$$

**çµŒæ¸ˆå­¦çš„è§£é‡ˆ**: 
- Demand system ã§ã®ä¾¡æ ¼å¤‰åŒ–ã«å¯¾ã™ã‚‹éœ€è¦ã®å¤‰åŒ–ï¼ˆSlutskyæ–¹ç¨‹å¼ï¼‰
- ã“ã“ã“ã§ã¯ã€ŒçŠ¶æ…‹ç©ºé–“ã®ä½“ç©ã®å¤‰åŒ–ã€ã‚’è¿½è·¡

---

## 5. ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹è¿‘ä¼¼

### 5.1 ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãƒ¢ãƒ‡ãƒ«

`Ï†` ã‚’æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¿‘ä¼¼ï¼š

$$
\phi(t, s_t, s_0; \theta) = \eta(t; \theta_\eta) \cdot Q(s_0; \theta_Q) \cdot s_t
$$

#### (A) `Q(s_0; Î¸_Q)`: è¡Œåˆ—å€¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ

```
s_0 âˆˆ â„^m â†’ [Linear â†’ Tanh]Ã—3 â†’ Linear â†’ Q âˆˆ â„^{mÃ—m}
```

**å…¥åŠ›**: åˆæœŸçŠ¶æ…‹ `s_0`  
**å‡ºåŠ›**: `m Ã— m` è¡Œåˆ—

**å½¹å‰²**: `s_0` ã«å¿œã˜ã¦ã€Œã©ã®æ–¹å‘ã«é€²ã‚€ã‹ã€ã‚’æ±ºå®š

#### (B) `Î·(t; Î¸_Î·)`: ã‚¹ã‚«ãƒ©ãƒ¼å€¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ

```
t âˆˆ [0,1] â†’ [Linear â†’ Tanh]Ã—2 â†’ Linear â†’ Tanh â†’ Î· âˆˆ [-c, c]
```

**å…¥åŠ›**: æ™‚åˆ» `t`  
**å‡ºåŠ›**: ã‚¹ã‚«ãƒ©ãƒ¼

**å½¹å‰²**: æ™‚é–“çµŒéã«å¿œã˜ã¦ã€Œã©ã‚Œãã‚‰ã„é€Ÿãé€²ã‚€ã‹ã€ã‚’èª¿æ•´

### 5.2 ãªãœã“ã®æ§‹é€ ï¼Ÿ

#### ç·šå½¢æ€§ã®é‡è¦æ€§

`Ï†` ãŒ `s_t` ã«é–¢ã—ã¦ç·šå½¢ï¼š

$$
\phi(t, \alpha s_t, s_0) = \alpha \cdot \phi(t, s_t, s_0)
$$

**åˆ©ç‚¹**:
- ODEã®è§£ãŒå®‰å®š
- ç†è«–çš„ãªä¿è¨¼ï¼ˆLipschitzé€£ç¶šæ€§ï¼‰

**çµŒæ¸ˆå­¦çš„é¡æ¨**: ç·šå½¢éœ€è¦ã‚·ã‚¹ãƒ†ãƒ ï¼ˆAIDS modelãªã©ï¼‰

#### åˆæœŸçŠ¶æ…‹ä¾å­˜

`Q` ãŒ `s_0` ã«ä¾å­˜ï¼š

$$
Q(s_0) \neq Q(s_0')
$$

**ç›´æ„Ÿ**: 
- ç•°ãªã‚‹åˆæœŸåˆ†å¸ƒã‹ã‚‰ç•°ãªã‚‹ãƒãƒ³ãƒ‰ãƒ«ã‚’ç”Ÿæˆã—ãŸã„
- `s_0 = [0.8, 0.2, 0.1, ...]` â†’ ã€Œæœ€åˆã®å•†å“ã‚’å«ã‚€ãƒãƒ³ãƒ‰ãƒ«ã€ã«å‘ã‹ã†
- `s_0 = [0.1, 0.9, 0.8, ...]` â†’ ã€Œ2-3ç•ªç›®ã®å•†å“ã‚’å«ã‚€ãƒãƒ³ãƒ‰ãƒ«ã€ã«å‘ã‹ã†

**çµŒæ¸ˆå­¦çš„é¡æ¨**: State-dependent transition probabilitiesï¼ˆMarkovéç¨‹ï¼‰

---

## 6. å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

### 6.1 ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹

å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ï¼š

1. **åˆæœŸã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: `s_0 ~ Î£ w_d N(Î¼_d, Ïƒ_dÂ² I)`
2. **é›¢æ•£åŒ–**: `s = round(s_0)` ï¼ˆè¦ç´ ã”ã¨ã« `s_i = ğŸ™{s_{0,i} â‰¥ 0.5}`ï¼‰
3. **ãƒã‚¤ã‚ºä»˜åŠ **: `s_T ~ N(s, Ïƒ_zÂ² I)`
4. **æ™‚åˆ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: `t ~ Uniform[0,1]`
5. **ç·šå½¢è£œé–“**: `s_t = t Â· s_T + (1-t) Â· s_0`

**çµ±è¨ˆçš„è§£é‡ˆ**: 
- `s_0, s_T, t` ã¯ i.i.d. ã‚µãƒ³ãƒ—ãƒ«ï¼ˆMonte Carloæ³•ï¼‰
- `s_t` ã¯ conditional distribution `p(s_t | s_0, s_T, t)`

### 6.2 å‹¾é…é™ä¸‹æ³•

æå¤±é–¢æ•°ï¼š

$$
\mathcal{L}(\theta) = \mathbb{E}_{s_0, s_T, t} \left[ \| (s_T - s_0) - \phi(t, s_t, s_0; \theta) \|^2 \right]
$$

å‹¾é…ï¼š

$$
\nabla_\theta \mathcal{L} = \mathbb{E} \left[ -2(s_T - s_0 - \phi) \cdot \nabla_\theta \phi \right]
$$

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **ï¼ˆSGDï¼‰:

```
for iteration = 1 to N:
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    s_0 ~ Î¼â‚€
    s = round(s_0)
    s_T ~ N(s, Ïƒ_zÂ² I)
    t ~ U[0,1]
    s_t = tÂ·s_T + (1-t)Â·s_0
    
    # æå¤±è¨ˆç®—
    target = s_T - s_0
    pred = Ï†(t, s_t, s_0; Î¸)
    loss = ||target - pred||Â²
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
    Î¸ â† Î¸ - Î· âˆ‡_Î¸ loss
```

**çµŒæ¸ˆå­¦çš„é¡æ¨**: 
- Simulated Method of Moments (SMM)
- Neural network = flexible functional form

### 6.3 æ­£å‰‡åŒ–é …

éå­¦ç¿’ã‚’é˜²ããŸã‚ã®è¿½åŠ é …ï¼š

#### (A) Jacobian Penaltyï¼ˆè»Œé“çŸ­ç¸®ï¼‰

$$
\lambda_j \cdot \mathbb{E} \left[ \left\| \frac{\partial \phi}{\partial s_t} \right\|_F^2 \right]
$$

**ç›´æ„Ÿ**: `Ï†` ã®å¤‰åŒ–ã‚’æ»‘ã‚‰ã‹ã«ã™ã‚‹ï¼ˆéåº¦ã«è¤‡é›‘ãªè»Œé“ã‚’é˜²ãï¼‰

#### (B) Kinetic Energyï¼ˆãƒ™ã‚¯ãƒˆãƒ«å ´åˆ¶å¾¡ï¼‰

$$
\lambda_k \cdot \mathbb{E} \left[ \| \phi(t, s_t, s_0) \|^2 \right]
$$

**ç›´æ„Ÿ**: Flow ã®ã€Œé€Ÿåº¦ã€ã‚’åˆ¶é™ï¼ˆç™ºæ•£ã‚’é˜²ãï¼‰

#### (C) Trace Penaltyï¼ˆç™ºæ•£æŠ‘åˆ¶ï¼‰

$$
\lambda_{tr} \cdot \mathbb{E} \left[ (\text{Tr}[Q(s_0)])^2 \right]
$$

**ç›´æ„Ÿ**: å¯†åº¦é‡ã¿ã®å¤‰åŒ–ã‚’æŠ‘åˆ¶ï¼ˆæ•°å€¤çš„å®‰å®šæ€§ï¼‰

---

## 7. æ¨è«–ï¼ˆãƒãƒ³ãƒ‰ãƒ«ç”Ÿæˆï¼‰

### 7.1 å­¦ç¿’å¾Œã®ä½¿ç”¨æ–¹æ³•

ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ `Î¸` ãŒå­¦ç¿’ã§ããŸã‚‰ã€ãƒãƒ³ãƒ‰ãƒ«ã‚’ç”Ÿæˆï¼š

```python
# 1. åˆæœŸåˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«
s_0 ~ Î£ w_d N(Î¼_d, Ïƒ_dÂ² I)

# 2. ODEã‚’ç©åˆ†
for i = 0 to N-1:
    s_{t_{i+1}} = s_{t_i} + Î”t Â· Ï†(t_i, s_{t_i}, s_0; Î¸)

# 3. é›¢æ•£åŒ–
s_i = ğŸ™{s_{T,i} â‰¥ 0.5}, i = 1, ..., m
```

**çµæœ**: `s âˆˆ {0,1}^m` ï¼ˆé›¢æ•£ãƒãƒ³ãƒ‰ãƒ«ï¼‰

### 7.2 å¤šæ§˜æ€§ã®ç¢ºä¿

ç•°ãªã‚‹ `s_0` ã‹ã‚‰ç•°ãªã‚‹ `s` ãŒç”Ÿæˆã•ã‚Œã‚‹ï¼š

```
s_0^(1) = [0.9, 0.1, 0.2, ...] â†’ s^(1) = [1, 0, 0, ...]
s_0^(2) = [0.1, 0.8, 0.9, ...] â†’ s^(2) = [0, 1, 1, ...]
s_0^(3) = [0.5, 0.5, 0.1, ...] â†’ s^(3) = [?, ?, 0, ...]  # ç¢ºç‡çš„
```

**çµŒæ¸ˆå­¦çš„æ„ç¾©**: 
- å¤šæ§˜ãªãƒãƒ³ãƒ‰ãƒ«ã‚’æä¾› â†’ ç•°è³ªãªè²·ã„æ‰‹ã«å¯¾å¿œ
- ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãƒ‡ã‚¶ã‚¤ãƒ³ã®ã€Œãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€æ¦‚å¿µã¨ä¸€è‡´

---

## 8. BundleFlowã«ãŠã‘ã‚‹2æ®µéšå­¦ç¿’

### 8.1 Stage 1: Rectified Flow ã®å­¦ç¿’

**å…¥åŠ›ãƒ‡ãƒ¼ã‚¿**: ãªã—ï¼ˆæ•™å¸«ãªã—å­¦ç¿’ï¼‰

**å­¦ç¿’å¯¾è±¡**: 
- `Î¼_d` ï¼ˆå„æˆåˆ†ã®ä¸­å¿ƒï¼‰
- `w_d` ï¼ˆæ··åˆé‡ã¿ï¼‰
- `Î¸_Q, Î¸_Î·` ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

**ç›®çš„**: å¤šæ§˜ãªãƒãƒ³ãƒ‰ãƒ«ã‚’ç”Ÿæˆã§ãã‚‹åˆ†å¸ƒã‚’å­¦ç¿’

$$
\min_{\mu, w, \theta} \mathbb{E}_{s_0 \sim \mu_0} \left[ \| (s_T - s_0) - \phi(t, s_t, s_0; \theta) \|^2 \right]
$$

**çµŒæ¸ˆå­¦çš„è§£é‡ˆ**: 
- ã€Œå¸‚å ´ã«å‡ºå›ã‚‹å•†å“ã®ç¨®é¡ã€ã‚’æ±ºã‚ã‚‹
- è²·ã„æ‰‹ã®å—œå¥½ã‚’**ã¾ã è€ƒæ…®ã—ã¦ã„ãªã„**ï¼ˆç´”ç²‹ã«æŠ€è¡“çš„ãªå•é¡Œï¼‰

### 8.2 Stage 2: ãƒ¡ãƒ‹ãƒ¥ãƒ¼æœ€é©åŒ–

**å…¥åŠ›ãƒ‡ãƒ¼ã‚¿**: è²·ã„æ‰‹ã®è©•ä¾¡å€¤åˆ†å¸ƒ `V = {vâ‚, ..., v_n}`

**å­¦ç¿’å¯¾è±¡**: 
- `K` å€‹ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼è¦ç´ ã€å„ã€…ãŒï¼š
  - `Î²_k`: ä¾¡æ ¼
  - `Î¼_d^(k)`: åˆæœŸåˆ†å¸ƒï¼ˆãƒãƒ³ãƒ‰ãƒ«ç”Ÿæˆç”¨ï¼‰
  - `w_d^(k)`: æ··åˆé‡ã¿

**ç›®çš„**: åç›Šã‚’æœ€å¤§åŒ–

$$
\max_{\beta, \mu, w} \mathbb{E}_{v \sim \mathcal{V}} \left[ \sum_{k=1}^K Z_k(v) \cdot \beta_k \right]
$$

where `Z_k(v)` ã¯è²·ã„æ‰‹ `v` ãŒãƒ¡ãƒ‹ãƒ¥ãƒ¼ `k` ã‚’é¸ã¶ç¢ºç‡ï¼ˆsoftmax or Gumbel-Softmaxï¼‰

**çµŒæ¸ˆå­¦çš„è§£é‡ˆ**:
- Myerson ã®æœ€é©ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³è¨­è¨ˆ
- Price discriminationï¼ˆä¾¡æ ¼å·®åˆ¥ï¼‰
- Menu designï¼ˆRochet & ChonÃ© 1998ï¼‰

### 8.3 ãªãœ2æ®µéšï¼Ÿ

#### æŠ€è¡“çš„ç†ç”±

1. **æ¢ç´¢ç©ºé–“ã®å‰Šæ¸›**: Stage 1ã§ã€Œåˆç†çš„ãªãƒãƒ³ãƒ‰ãƒ«ç©ºé–“ã€ã‚’å­¦ç¿’
2. **åˆæœŸåŒ–**: Stage 2ã§è‰¯ã„åˆæœŸå€¤ã‚’æä¾›
3. **å®‰å®šæ€§**: 2ã¤ã®ç›®çš„ã‚’åˆ†é›¢ â†’ å­¦ç¿’ãŒå®‰å®š

#### çµŒæ¸ˆå­¦çš„é¡æ¨

**2æ®µéšæ¨å®š** (Two-step estimation):
1. **ç¬¬1æ®µéš**: æ§‹é€ çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®š
2. **ç¬¬2æ®µéš**: çµŒæ¸ˆçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šï¼ˆç¬¬1æ®µéšã‚’å›ºå®šï¼‰

ä¾‹: Heckman selection model, Control function approach

---

## 9. ç†è«–çš„æ­£å½“åŒ–

### 9.1 ãªãœç›´ç·šçš„ãªçµŒè·¯ï¼Ÿ

**Rectified Flow ã®ä¸»å¼µ**: 
> æœ€é©ãªè¼¸é€ã¯ç›´ç·šçš„

ã“ã‚Œã¯**Optimal Transportç†è«–**ã«åŸºã¥ãï¼š

$$
\min_{\gamma} \mathbb{E}_{(s_0, s_T) \sim \gamma} \left[ \| s_T - s_0 \|^2 \right]
$$

subject to: `Î³` ã®å‘¨è¾ºåˆ†å¸ƒãŒ `Î¼â‚€` ã¨ `s` ã®åˆ†å¸ƒã«ä¸€è‡´

**Monge-Kantorovichå•é¡Œ**ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ï¼ˆ`LÂ²` ã‚³ã‚¹ãƒˆï¼‰

#### çµŒæ¸ˆå­¦ã§ã®é¡ä¼¼æ¦‚å¿µ

- **Walrasian equilibrium**: æœ€ã‚‚åŠ¹ç‡çš„ãªè³‡æºé…åˆ†
- **Lindahl prices**: å…¬å…±è²¡ã®æœ€é©ä¾¡æ ¼
- **Matching theory**: å®‰å®šãƒãƒƒãƒãƒ³ã‚°ï¼ˆGale-Shapleyï¼‰

### 9.2 Flow Matching ã®èƒŒæ™¯

Rectified Flowã¯**Flow Matching**ã®ä¸€ç¨®ï¼š

1. **Normalizing Flows** (Rezende & Mohamed 2015): å¯é€†å¤‰æ›ã§è¤‡é›‘ãªåˆ†å¸ƒã‚’ç”Ÿæˆ
2. **Continuous Normalizing Flows** (Chen et al. 2018): ODEã§è¡¨ç¾
3. **Flow Matching** (Lipman et al. 2023): ç›´æ¥ODEã‚’å­¦ç¿’
4. **Rectified Flow** (Liu et al. 2022): ç›´ç·šçµŒè·¯ã«çŸ¯æ­£

**çµ±è¨ˆå­¦ã§ã®é¡ä¼¼**: 
- Normalizing constants in Bayesian inference
- Importance sampling ã® reweighting

---

## 10. å®Ÿè£…ä¸Šã®æŠ€è¡“çš„è©³ç´°

### 10.1 Spectral Normalization

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ `Q` ã®é‡ã¿ã«åˆ¶ç´„ï¼š

$$
\| W \|_2 \leq 1 \quad (\text{æœ€å¤§ç‰¹ç•°å€¤ã‚’1ã«åˆ¶é™})
$$

**ç›®çš„**: Lipschitz é€£ç¶šæ€§ã‚’ä¿è¨¼

$$
\| Q(s_0) - Q(s_0') \| \leq L \| s_0 - s_0' \|
$$

**çµŒæ¸ˆå­¦çš„é¡æ¨**: 
- Smoothness restriction in nonparametric estimation
- Bounded rationality ï¼ˆé™å®šåˆç†æ€§ï¼‰

### 10.2 å­¦ç¿’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

#### Cosine Annealing (å­¦ç¿’ç‡èª¿æ•´)

$$
\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min}) \left(1 + \cos\left(\frac{\pi t}{T}\right)\right)
$$

**ç›´æ„Ÿ**: å­¦ç¿’ã®çµ‚ç›¤ã§å°ã•ã„å­¦ç¿’ç‡ â†’ ç²¾å¯†ãªèª¿æ•´

**çµŒæ¸ˆå­¦ã§ã®é¡ä¼¼**: Sequential testing with declining significance levels

### 10.3 æ•°å€¤çš„å®‰å®šæ€§

#### Log-Sum-Exp Trick

é‡ã¿ä»˜ãå’Œã‚’è¨ˆç®—ã™ã‚‹éš›ï¼š

$$
\sum_{d=1}^D w_d \cdot \exp(\text{log\_density}_d) \cdot v_d
$$

æ•°å€¤çš„ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ãï¼š

$$
M = \max_d \text{log\_weight}_d
$$

$$
\text{sum} = \exp(M) \cdot \sum_d \exp(\text{log\_weight}_d - M) \cdot v_d
$$

**çµ±è¨ˆå­¦ã§ã®é¡ä¼¼**: Logsumexp in softmax regression, log-likelihood computation

---

## 11. Straight-Through Estimator (STE)

### 11.1 é›¢æ•£åŒ–ã®å‹¾é…å•é¡Œ

**å•é¡Œ**: `round` é–¢æ•°ã¯å¾®åˆ†ä¸å¯èƒ½

$$
s = \mathbb{1}\{s_T \geq 0.5\}
$$

å‹¾é…:

$$
\frac{\partial s}{\partial s_T} = 0 \quad (\text{ã»ã¼ã™ã¹ã¦ã®ç‚¹ã§})
$$

**çµæœ**: å­¦ç¿’ãŒé€²ã¾ãªã„ï¼

### 11.2 STEã®è§£æ±ºç­–

**Forwardæ™‚**: é›¢æ•£çš„ãªæ“ä½œã‚’å®Ÿè¡Œ

$$
s^{\text{forward}} = \mathbb{1}\{s_T \geq 0.5\}
$$

**Backwardæ™‚**: é€£ç¶šçš„ãªè¿‘ä¼¼ã®å‹¾é…ã‚’ä½¿ç”¨

$$
\frac{\partial s}{\partial s_T}\Big|_{\text{backward}} = \frac{\partial}{\partial s_T} \sigma\left(\frac{s_T - 0.5}{\tau}\right)
$$

where `Ïƒ(x) = 1/(1 + e^{-x})` ã¯sigmoidé–¢æ•°ã€‚

**PyTorchå®Ÿè£…**:

```python
s_hard = (s_T >= 0.5).float()
s_soft = torch.sigmoid((s_T - 0.5) / tau)
s = s_hard - s_soft.detach() + s_soft
#   ^^^^^^   ^^^^^^^^^^^^^^^^   ^^^^^^
#   ä½¿ã†å€¤    å‹¾é…ã‚’ãƒ–ãƒ­ãƒƒã‚¯    å‹¾é…ã®å…ƒ
```

### 11.3 ãƒã‚¤ã‚¢ã‚¹ã¨å®Ÿå‹™çš„æˆåŠŸ

#### ç†è«–çš„å•é¡Œ

STEã¯**ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹æ¨å®šé‡**ï¼š

$$
\mathbb{E}[\nabla_\theta^{\text{STE}}] \neq \nabla_\theta \mathbb{E}[\mathcal{L}]
$$

çœŸã®å‹¾é…ã¨ç•°ãªã‚‹ï¼

#### ãªãœæ©Ÿèƒ½ã™ã‚‹ã‹

1. **ä½åˆ†æ•£**: Policy Gradientã‚ˆã‚Šåˆ†æ•£ãŒå°ã•ã„
2. **å±€æ‰€çš„è¿‘ä¼¼**: `f_hard â‰ˆ f_soft` when `Ï„` is small
3. **å¤§æ•°ã®æ³•å‰‡**: ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„ã¨ãƒã‚¤ã‚¢ã‚¹ãŒå¹³å‡åŒ–
4. **çµŒé¨“çš„æˆåŠŸ**: VQ-VAE, Binarized NNsãªã©ã§å®Ÿè¨¼æ¸ˆã¿

**è¨ˆé‡çµŒæ¸ˆå­¦ã§ã®é¡æ¨**:
- Quasi-maximum likelihood: èª¤ã£ãŸå°¤åº¦é–¢æ•°ã§ã‚‚ä¸€è‡´æ¨å®šé‡
- GMM: ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆæ¡ä»¶ã®è¿‘ä¼¼

---

## 12. Stage 2ã§ã®Gumbel-Softmax

### 12.1 ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠã®å•é¡Œ

è²·ã„æ‰‹ `v` ã¯åŠ¹ç”¨æœ€å¤§åŒ–ï¼š

$$
k^* = \arg\max_{k \in [K]} u^{(k)}(v)
$$

where `u^(k) = v(S_k) - Î²_k`.

**å•é¡Œ**: `argmax` ã¯å¾®åˆ†ä¸å¯èƒ½

### 12.2 Gumbel-Max Trick

**å®šç†**: ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†å¸ƒã¯ä»¥ä¸‹ã¨ç­‰ä¾¡ï¼š

$$
k \sim \text{Categorical}(p) \iff k = \arg\max_i (\log p_i + G_i)
$$

where `G_i ~ Gumbel(0,1)` ã¯ç‹¬ç«‹ã€‚

**Gumbelåˆ†å¸ƒ**:

$$
P(G \leq g) = \exp(-\exp(-g))
$$

**çµŒæ¸ˆå­¦çš„è§£é‡ˆ**: 
- Logit model ã®ç¢ºç‡çš„åŠ¹ç”¨ç†è«–
- Random utility maximization (McFadden 1974)

### 12.3 Gumbel-Softmax

`argmax` ã‚’ `softmax` ã§é€£ç¶šç·©å’Œï¼š

$$
y_k = \frac{\exp\left( (u^{(k)} + G_k) / \tau \right)}{\sum_j \exp\left( (u^{(j)} + G_j) / \tau \right)}
$$

**æ¸©åº¦ `Ï„`**:
- `Ï„ â†’ 0`: one-hotï¼ˆé›¢æ•£ï¼‰
- `Ï„ â†’ âˆ`: ä¸€æ§˜åˆ†å¸ƒï¼ˆé€£ç¶šï¼‰

**+ STE**:

```python
# Forward: hardé¸æŠï¼ˆãƒ†ã‚¹ãƒˆã¨åŒã˜ï¼‰
k* = argmax(u + G)
revenue = Î²[k*] if u[k*] >= 0 else 0

# Backward: softã®å‹¾é…ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰
y_soft = softmax((u + G) / Ï„)
gradient uses: âˆ‚y_soft/âˆ‚u
```

### 12.4 ãªãœSoftmax relaxationã¯å¤±æ•—ã—ãŸã‹

#### å¾“æ¥ï¼ˆSoftmax relaxationï¼‰

```
Forward:  Z = softmax(u / Î»)  # é€£ç¶š
Backward: Z = softmax(u / Î»)  # é€£ç¶š
Test:     k* = argmax(u)      # é›¢æ•£ â† ä¸ä¸€è‡´ï¼
```

**å•é¡Œ**: Training objective â‰  Test objective

#### Gumbel + STE

```
Forward:  k* = argmax(u + G)  # é›¢æ•£ï¼ˆãƒ†ã‚¹ãƒˆã¨åŒã˜ï¼‰
Backward: y = softmax(...)     # é€£ç¶šï¼ˆå‹¾é…è¨ˆç®—ï¼‰
Test:     k* = argmax(u)       # é›¢æ•£ âœ“
```

**åˆ©ç‚¹**: Training â‰ˆ Test

**çµŒæ¸ˆå­¦çš„æ•™è¨“**:
> Estimating equation should match the economic model

- Simulated MLE: Simulated distribution â‰ˆ True distribution
- Indirect inference: Auxiliary model â‰ˆ Structural model

---

## 13. è¨ˆç®—è¤‡é›‘åº¦

### 13.1 Stage 1

å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:

1. **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: `O(B Â· m)`
2. **ODEç©åˆ†**: `O(B Â· mÂ² Â· N_steps)` ï¼ˆè¡Œåˆ—ç© `Q Â· s_t`ï¼‰
3. **å‹¾é…è¨ˆç®—**: `O(B Â· mÂ² Â· N_params)`

**Total**: `O(B Â· mÂ² Â· N_steps)` per iteration

`m=50, B=512, N_steps=25` ã®å ´åˆ: ç´„3200ä¸‡æ¼”ç®—/iteration

### 13.2 Stage 2

å„ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:

1. **ãƒãƒ³ãƒ‰ãƒ«ç”Ÿæˆ**: `K Â· D` å›ã®ODEç©åˆ† â†’ `O(K Â· D Â· mÂ² Â· N_steps)`
2. **åŠ¹ç”¨è¨ˆç®—**: `B` å€‹ã®è²·ã„æ‰‹ Ã— `K` å€‹ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ â†’ `O(B Â· K Â· D)`
3. **Softmax/Gumbel**: `O(B Â· K)`

**Total**: `O(K Â· D Â· mÂ² Â· N_steps + B Â· K Â· D)`

`m=50, K=512, D=8, B=128, N_steps=25` ã®å ´åˆ: ç´„50å„„æ¼”ç®—/iteration

**GPUã®é‡è¦æ€§**: ãƒãƒƒãƒå‡¦ç†ã§ä¸¦åˆ—åŒ– â†’ å®Ÿéš›ã«ã¯æ•°ç§’

---

## 14. ç†è«–çš„é™ç•Œã¨æ‹¡å¼µå¯èƒ½æ€§

### 14.1 ç¾åœ¨ã®åˆ¶ç´„

#### (A) é›¢æ•£åŒ–ã®æå¤±

`round(s_T)` ã§æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹ï¼š

```
s_T = [0.49, 0.51] â†’ s = [0, 1]  # å¾®å¦™ãªå·®ãŒæ¶ˆãˆã‚‹
s_T = [0.01, 0.99] â†’ s = [0, 1]  # åŒã˜çµæœ
```

**è§£æ±ºç­–ï¼ˆå°†æ¥ï¼‰**: Soft rounding ã®æ”¹å–„ã€STEã®ç²¾ç·»åŒ–

#### (B) åˆæœŸåˆ†å¸ƒã®è¨­è¨ˆ

`Î¼â‚€` ã®é¸æŠã«æ£æ„æ€§ï¼š

- Gaussianæ··åˆã‚’ä»®å®š
- æ”¯æŒã‚µã‚¤ã‚º `D` ã¯å›ºå®š

**è§£æ±ºç­–**: ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®åˆæœŸåˆ†å¸ƒè¨­è¨ˆ

### 14.2 çµŒæ¸ˆå­¦ã¸ã®å¿œç”¨å¯èƒ½æ€§

#### (A) å¤šå•†å“ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ä»¥å¤–

- **Matching markets**: åŒ»å¸«-ç—…é™¢ã€å­¦æ ¡é¸æŠ
- **Voting systems**: æŠ•ç¥¨ãƒ«ãƒ¼ãƒ«è¨­è¨ˆ
- **Contract design**: è¤‡é›‘ãªå¥‘ç´„ã®æœ€é©åŒ–

#### (B) è©•ä¾¡å€¤æ¨å®šã¨ã®çµ±åˆ

ç¾åœ¨ã¯ `v` ãŒæ—¢çŸ¥ã¨ä»®å®šã€‚å®Ÿå‹™ã§ã¯ï¼š

```
ãƒ‡ãƒ¼ã‚¿ â†’ è©•ä¾¡å€¤æ¨å®š â†’ ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è¨­è¨ˆ
```

**ææ¡ˆ**: End-to-endå­¦ç¿’

---

## 15. ã¾ã¨ã‚

### Rectified Flowã®3ã¤ã®æŸ±

1. **ODE-based generation**: é€£ç¶šåˆ†å¸ƒã‹ã‚‰é›¢æ•£æ§‹é€ ã¸
2. **Learnable transformation**: æ·±å±¤å­¦ç¿’ã§æŸ”è»Ÿãªå†™åƒã‚’ç²å¾—
3. **Straight-through estimation**: é›¢æ•£æ“ä½œã®å¾®åˆ†å¯èƒ½è¿‘ä¼¼

### çµŒæ¸ˆå­¦ã¨ã®æ¥ç‚¹

| æ¦‚å¿µ | Rectified Flow | çµŒæ¸ˆå­¦ |
|------|---------------|--------|
| çŠ¶æ…‹é·ç§» | `s_0 â†’ s_T` via ODE | Tatonnement, adjustment process |
| ç¢ºç‡çš„é¸æŠ | Gumbel-Max Trick | Random utility model (Logit) |
| æœ€é©åŒ– | SGD on neural net | Simulated methods (SMM, MSM) |
| æ­£å‰‡åŒ– | Jacobian/Kinetic penalty | Smoothness restriction |
| 2æ®µéšå­¦ç¿’ | Stage 1 + Stage 2 | Two-step estimation |

### å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆ

1. **ç¢ºç‡åˆ†å¸ƒã®æ­£è¦åŒ–**: ã™ã¹ã¦ `softmax` ã§ä¿è¨¼ âœ“
2. **æ•°å€¤å®‰å®šæ€§**: Log-sum-exp, clipping âœ“
3. **å‹¾é…ã®æµã‚Œ**: STE, Gumbel-Softmaxã§ç¢ºä¿ âœ“

### ä»Šå¾Œã®ç ”ç©¶æ–¹å‘

1. **ç†è«–çš„ä¿è¨¼**: STEã®ãƒã‚¤ã‚¢ã‚¹è©•ä¾¡ã€åæŸæ¡ä»¶
2. **å®Ÿãƒ‡ãƒ¼ã‚¿é©ç”¨**: CATS, FCC spectrum auction ãƒ‡ãƒ¼ã‚¿
3. **ä»–æ‰‹æ³•ã¨ã®æ¯”è¼ƒ**: RegretNet, RochetNet, VCG
4. **è¨ˆç®—åŠ¹ç‡åŒ–**: è¿‘ä¼¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€åˆ†æ•£å­¦ç¿’

---

## 16. å‚è€ƒæ–‡çŒ®

### Rectified Flowé–¢é€£

1. **Liu, X., Gong, C., & Liu, Q. (2022).** Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow. *arXiv:2209.03003*.
   - Rectified Flowã®åŸè«–æ–‡

2. **Lipman, Y., Chen, R. T. Q., et al. (2023).** Flow Matching for Generative Modeling. *ICLR 2023*.
   - Flow Matchingã®ä¸€èˆ¬çš„æ çµ„ã¿

3. **Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018).** Neural Ordinary Differential Equations. *NeurIPS 2018*.
   - Neural ODEã®åŸºç¤

### Gumbel-Softmax & STE

4. **Jang, E., Gu, S., & Poole, B. (2017).** Categorical Reparameterization with Gumbel-Softmax. *ICLR 2017*.
   - Gumbel-Softmaxã®ææ¡ˆ

5. **Bengio, Y., LÃ©onard, N., & Courville, A. (2013).** Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. *arXiv:1308.3432*.
   - Straight-Through Estimatorã®ææ¡ˆ

### çµŒæ¸ˆå­¦ãƒ»æœ€é©åŒ–ç†è«–

6. **Villani, C. (2009).** Optimal Transport: Old and New. *Springer*.
   - Optimal Transportç†è«–ã®æ•™ç§‘æ›¸

7. **McFadden, D. (1974).** Conditional Logit Analysis of Qualitative Choice Behavior. *Frontiers in Econometrics*.
   - Random utility modelã®åŸºç¤

8. **Myerson, R. B. (1981).** Optimal Auction Design. *Mathematics of Operations Research*, 6(1), 58-73.
   - æœ€é©ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è¨­è¨ˆ

9. **Rochet, J. C., & ChonÃ©, P. (1998).** Ironing, Sweeping, and Multidimensional Screening. *Econometrica*, 66(4), 783-826.
   - å¤šæ¬¡å…ƒã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

### BundleFlow

10. **Wang, T., Jiang, Y., & Parkes, D. C. (2025).** BundleFlow: Deep Menus for Combinatorial Auctions by Diffusion-Based Optimization. *arXiv:2502.15283*.
    - BundleFlowã®åŸè«–æ–‡

---

## ä»˜éŒ²A: ä¸»è¦ãªæ•°å¼ä¸€è¦§

### Stage 1: Rectified Flowå­¦ç¿’

**åˆæœŸåˆ†å¸ƒ**:

$$
s_0 \sim \mu_0 = \sum_{d=1}^D w_d \cdot \mathcal{N}(\mu_d, \sigma_d^2 I)
$$

**ãƒ™ã‚¯ãƒˆãƒ«å ´**:

$$
\phi(t, s_t, s_0) = \eta(t) \cdot Q(s_0) \cdot s_t
$$

**æå¤±é–¢æ•°**:

$$
\mathcal{L}_{\text{flow}} = \mathbb{E}_{s_0, t} \left[ \| (s_T - s_0) - \phi(t, s_t, s_0) \|^2 \right]
$$

where:
- `s = round(s_0)`
- `s_T ~ N(s, Ïƒ_zÂ² I)`
- `s_t = t Â· s_T + (1-t) Â· s_0`

**æ­£å‰‡åŒ–**:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{flow}} + \lambda_j \mathcal{R}_{\text{jacobian}} + \lambda_k \mathcal{R}_{\text{kinetic}} + \lambda_{tr} \mathcal{R}_{\text{trace}}
$$

### Stage 2: ãƒ¡ãƒ‹ãƒ¥ãƒ¼æœ€é©åŒ–

**åŠ¹ç”¨é–¢æ•°**:

$$
u^{(k)}(v) = \sum_{d=1}^D w_d^{(k)} \cdot v(s_d^{(k)}) \cdot \rho_d^{(k)} - \beta^{(k)}
$$

where:
- `s_d^(k)` = Flow ã§ç”Ÿæˆã•ã‚ŒãŸãƒãƒ³ãƒ‰ãƒ«ï¼ˆ`Î¼_d^(k)` ã‹ã‚‰ï¼‰
- `Ï_d^(k)` = å¯†åº¦é‡ã¿ `exp(-Tr[Q(Î¼_d^(k))] âˆ« Î·)`
- `Î²^(k)` = ä¾¡æ ¼

**åç›Šï¼ˆGumbel-Softmaxç‰ˆï¼‰**:

$$
\text{Revenue} = \mathbb{E}_{v, G} \left[ \beta_{k^*} \cdot \mathbb{1}\{u^{(k^*)} \geq 0\} \right]
$$

where `k* = argmax_k (u^(k) + G_k)`.

**æå¤±é–¢æ•°**:

$$
\mathcal{L}_{\text{revenue}} = -\text{Revenue}
$$

---

## ä»˜éŒ²B: ç”¨èªå¯¾ç…§è¡¨

| æŠ€è¡“ç”¨èª | çµŒæ¸ˆå­¦çš„è§£é‡ˆ |
|---------|------------|
| Neural ODE | Continuous-time dynamic system |
| Flow Matching | Optimal transport problem |
| Rectified Flow | Monge transportation |
| Gumbel-Softmax | Multinomial logit with error |
| Straight-Through | Quasi-likelihood approximation |
| Spectral Norm | Lipschitz constraint |
| Log-sum-exp | Numerical stable logsumexp |
| Batch normalization | Cross-sectional standardization |
| Gradient clipping | Bounded learning step |
| Cosine annealing | Adaptive step size |
| Mixture of Gaussians | Latent class model |
| Density weight | Jacobian determinant |
| Softmax temperature | Choice precision (McFadden) |
| Menu element | Contract offering (Rochet-ChonÃ©) |
| Utility | Indirect utility function |
| IR constraint | Participation constraint |
| Revenue | Expected payment |

---

**Document Author**: Technical explanation for economists  
**Date**: October 12, 2025  
**Target Audience**: Economists familiar with statistics but new to machine learning  
**Prerequisites**: Probability theory, optimization, basic linear algebra

